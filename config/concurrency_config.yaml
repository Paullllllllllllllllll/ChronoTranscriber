# =============================================================================
# Concurrency Configuration
# =============================================================================
# Controls parallel processing, API retry behavior, and token usage limits.
#
# Used by:
#   - modules/core/workflow.py (concurrency_limit, delay_between_tasks)
#   - modules/llm/batch/batching.py (service_tier, batch_chunk_size)
#   - modules/llm/providers/*.py (retry.attempts)
#   - modules/core/token_guard.py (daily_token_limit)
# =============================================================================

concurrency:
  transcription:
    # Maximum concurrent LLM API calls during synchronous transcription.
    # Higher values = faster processing but more API load.
    # Recommended: 20-50 for most use cases; up to 1500 for high-throughput scenarios.
    concurrency_limit: 1500

    # Delay (seconds) between starting each transcription task.
    # Helps avoid rate limiting; set to 0 for maximum throughput.
    delay_between_tasks: 0

    # OpenAI service tier for Responses API (synchronous mode only).
    # Values: 'auto', 'default', 'flex', 'priority'
    # Note: 'flex' is synchronous-only; batch processing auto-converts to 'auto'.
    service_tier: flex

    # Number of images per batch file when using OpenAI Batch API.
    # Larger chunks = fewer API calls but larger file uploads.
    batch_chunk_size: 50

    # Request timeout in seconds for each API call.
    # OpenAI's default SDK timeout is 10 minutes; flex processing can exceed this.
    # Set to 900 (15 minutes) as recommended by OpenAI for flex service tier.
    # LangChain will raise APITimeoutError and trigger retry on expiry.
    request_timeout: 900

    # Retry configuration for transient API errors (429, 5xx, timeouts).
    # LangChain handles exponential backoff internally; only 'attempts' is used.
    retry:
      # Maximum retry attempts before failing. LangChain applies exponential backoff.
      attempts: 10

# =============================================================================
# Daily Token Limit
# =============================================================================
# Tracks cumulative token usage across all LLM API calls and enforces a daily
# cap. When the limit is reached, processing pauses until midnight reset.
#
# Used by:
#   - modules/core/token_guard.py (limit enforcement)
#   - modules/infra/token_tracker.py (usage tracking)
#   - modules/core/workflow.py (check before each item)
# =============================================================================

daily_token_limit:
  # Master toggle. When false, no token tracking or limits are enforced.
  enabled: true

  # Maximum tokens allowed per day (input + output combined).
  # Processing pauses when this limit is reached; resumes at midnight.
  # Important: Files started when near the limit will be finished, you will be
  # charged for additional tokens used. If in doubt, set limit lower than
  # your budget.
  daily_tokens: 10_000_000
