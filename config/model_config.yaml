# model_config.yaml
transcription_model:
  # Provider selection: openai, anthropic, google, openrouter
  # If not specified, auto-detected from model name
  provider: anthropic
  
  # Model name - examples by provider (as of November 2025):
  #
  # OpenAI:
  #   - GPT-5.1: gpt-5.1, gpt-5.1-mini, gpt-5.1-nano (newest, adaptive thinking)
  #   - GPT-5: gpt-5, gpt-5-mini, gpt-5-nano
  #   - o-series: o4-mini, o3, o3-pro, o3-mini, o1, o1-pro, o1-mini
  #   - GPT-4.1: gpt-4.1, gpt-4.1-mini, gpt-4.1-nano (1M context)
  #   - GPT-4o: gpt-4o, gpt-4o-mini
  #
  # Anthropic:
  #   - Claude 4.5: claude-sonnet-4-5-20250929, claude-opus-4-5-XXXXXXXX, claude-haiku-4-5-XXXXXXXX
  #   - Claude 4.1: claude-opus-4-1-20250805
  #   - Claude 4: claude-sonnet-4-20250514, claude-opus-4-XXXXXXXX
  #   - Claude 3.5: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022
  #
  # Google:
  #   - Gemini 3: gemini-3-pro (state-of-the-art)
  #   - Gemini 2.5: gemini-2.5-pro, gemini-2.5-flash (adaptive thinking)
  #   - Gemini 2.0: gemini-2.0-flash
  #   - Gemini 1.5: gemini-1.5-pro, gemini-1.5-flash
  #
  # OpenRouter (200+ models via unified API):
  #   - openai/gpt-5.1, anthropic/claude-sonnet-4-5, google/gemini-3-pro
  #   - meta/llama-3.2-90b-vision, mistral/mistral-large
  #
  name: claude-haiku-4-5
  max_output_tokens: 128000 # maximum number of tokens the model can generate
  reasoning:                 # GPT-5-series and o3, o3-mini, o1, o1-mini, and o4-mini only.
  # We recommend using gpt-5-mini with high reasoning effort and medium verbosity
  # for a good balance of cost and quality.
    effort: low
  text:                      # GPT-5-series only
    verbosity: medium
  # classic sampler controls are used only for non-reasoning models like GPT-4o/4.1
  temperature: 0.01 # model temperature - higher values make the model more random
  top_p: 1.0 # nucleus sampling - controls the cumulative probability of tokens to consider
  frequency_penalty: 0.01 # frequency_penalty - penalizes repeated tokens;
  # flat reduction if a token has appeared more than once
  presence_penalty: 0.01 # presence_penalty - penalizes repeated tokens;
  # penalty increases with the number of times a token has appeared
