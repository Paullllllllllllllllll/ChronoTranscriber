# =============================================================================
# Model Configuration
# =============================================================================
# Configuration for the LLM used by the transcription pipeline.
#
# Used by:
#   - modules/llm/transcriber.py (LangChain-based multi-provider transcription)
#   - modules/llm/providers/*.py (provider-specific implementations)
#   - modules/llm/batch/batching.py (OpenAI Batch API transcription)
#   - modules/config/config_loader.py (model validation at startup)
#
# API Keys (environment variables):
#   - OpenAI: OPENAI_API_KEY
#   - Anthropic: ANTHROPIC_API_KEY
#   - Google: GOOGLE_API_KEY
#   - OpenRouter: OPENROUTER_API_KEY
# =============================================================================

transcription_model:
  # -------------------------------------------------------------------------
  # Provider Selection
  # -------------------------------------------------------------------------
  # Supported: openai, anthropic, google, openrouter
  # If omitted, auto-detected from model name (see modules/llm/providers/factory.py).
  provider: google

  # -------------------------------------------------------------------------
  # Model Identifier
  # -------------------------------------------------------------------------
  # For OpenRouter: use "provider/model" format (e.g., "anthropic/claude-sonnet-4-5")
  # For other providers: use the provider-native model ID
  #
  # Examples by provider:
  #
  # OpenAI:
  #   - GPT-5.1: gpt-5.1, gpt-5.1-mini, gpt-5.1-nano (newest, adaptive thinking)
  #   - GPT-5: gpt-5, gpt-5-mini, gpt-5-nano
  #   - o-series: o4-mini, o3, o3-pro, o3-mini, o1, o1-pro, o1-mini
  #   - GPT-4.1: gpt-4.1, gpt-4.1-mini, gpt-4.1-nano (1M context)
  #   - GPT-4o: gpt-4o, gpt-4o-mini
  #
  # Anthropic:
  #   - Claude 4.5: claude-sonnet-4-5-20250929, claude-opus-4-5-XXXXXXXX
  #   - Claude 4.1: claude-opus-4-1-20250805
  #   - Claude 4: claude-sonnet-4-20250514
  #   - Claude 3.5: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022
  #
  # Google:
  #   - Gemini 3: gemini-3-pro, gemini-3-flash-preview
  #   - Gemini 2.5: gemini-2.5-pro, gemini-2.5-flash
  #   - Gemini 2.0: gemini-2.0-flash, gemini-2.0-flash-exp
  #   - Gemini 1.5: gemini-1.5-pro, gemini-1.5-flash
  #
  # OpenRouter (200+ models via unified API):
  #   - openai/gpt-5.1, anthropic/claude-sonnet-4-5, google/gemini-3-pro
  #   - deepseek/deepseek-r1, meta/llama-3.2-90b-vision, mistral/pixtral-large
  name: gemini-2.0-flash-exp

  # -------------------------------------------------------------------------
  # Output Token Limit
  # -------------------------------------------------------------------------
  # Maximum tokens the model can generate per response.
  # For reasoning models (GPT-5, o-series), LangChain maps this to the
  # appropriate parameter name (max_completion_tokens).
  max_output_tokens: 128000

  # -------------------------------------------------------------------------
  # Image Input Validation
  # -------------------------------------------------------------------------
  # Set to true (default) to fail fast if the selected model lacks vision.
  # Set to false only for text-only pipelines (not typical for OCR).
  expects_image_inputs: true

  # -------------------------------------------------------------------------
  # Reasoning Controls
  # -------------------------------------------------------------------------
  # Applies to reasoning-capable models across all providers:
  #   - OpenAI GPT-5/o-series: Uses reasoning_effort parameter
  #   - Anthropic Claude 4.5/4.1: Uses extended thinking (budget_tokens)
  #   - Google Gemini 2.5/3: Uses thinking_level (low/high)
  #   - OpenRouter: Passes reasoning_effort to underlying models
  #
  # Values for effort: low | medium | high
  #   - low: Minimal reasoning overhead, faster responses
  #   - medium: Balanced reasoning depth (recommended)
  #   - high: Maximum reasoning depth, slower but more thorough
  reasoning:
    effort: low

  # Text output controls (OpenAI GPT-5 Batch API only).
  # Verbosity: concise | medium | verbose
  text:
    verbosity: medium

  # -------------------------------------------------------------------------
  # Sampler Controls
  # -------------------------------------------------------------------------
  # Supported on most non-reasoning chat models. For reasoning models
  # (GPT-5, o-series), these are automatically filtered by the provider.
  # Note: Anthropic extended thinking requires temperature: 1
  temperature: 1
  top_p: 1.0
  frequency_penalty: 0.01
  presence_penalty: 0.01

  # -------------------------------------------------------------------------
  # Service Tier (OpenAI only)
  # -------------------------------------------------------------------------
  # Configured in config/concurrency_config.yaml under:
  #   concurrency.transcription.service_tier: auto|default|priority|flex
  # Note: 'flex' is synchronous-only; batch processing auto-converts to 'auto'.
