# model_config.yaml
transcription_model:
  name: gpt-5-mini   # or o3, o1, gpt-4o, gpt-4.1, gpt-5, etc. (see modules/model_capabilities.py
  # and OpenAI API documentation)
  max_output_tokens: 128000 # maximum number of tokens the model can generate
  reasoning:                 # GPT-5-series and o3, o3-mini, o1, o1-mini, and o4-mini only.
  # We recommend using gpt-5-mini with high reasoning effort and medium verbosity
  # for a good balance of cost and quality.
    effort: high
  text:                      # GPT-5-series only
    verbosity: medium
  # classic sampler controls are used only for non-reasoning models like GPT-4o/4.1
  temperature: 0.01 # model temperature - higher values make the model more random
  top_p: 1.0 # nucleus sampling - controls the cumulative probability of tokens to consider
  frequency_penalty: 0.01 # frequency_penalty - penalizes repeated tokens;
  # flat reduction if a token has appeared more than once
  presence_penalty: 0.01 # presence_penalty - penalizes repeated tokens;
  # penalty increases with the number of times a token has appeared
