# ChronoTranscriber Evaluation Results

**Generated:** 20260221_080714

**Evaluation Method:** Page-level JSONL comparison â€” CER/WER computed after stripping all structural markup

## Models Evaluated

- **tesseract**: Tesseract OCR baseline (no LLM)
- **gpt_5.2_medium**: GPT-5.2 with medium reasoning
- **gpt_5_mini_medium**: GPT-5 Mini with medium reasoning
- **gemini_3.0_pro_medium**: Gemini 3 Pro Preview with medium reasoning
- **gemini_3.0_flash**: Gemini 3 Flash Preview
- **claude_sonnet_4.5_medium**: Claude Sonnet 4.5 with medium reasoning
- **claude_haiku_4.5_medium**: Claude Haiku 4.5 with medium reasoning

## Results by Category

| Model | Category | CER (%) | WER (%) | Ref Chars |
|-------|----------|---------|---------|-----------|
| claude_haiku_4.5_medium | address_books | 7.93 | 35.71 | 99,453 |
| claude_haiku_4.5_medium | bibliography | 7.16 | 13.34 | 64,693 |
| claude_haiku_4.5_medium | military_records | 34.78 | 64.82 | 2,898 |
| claude_sonnet_4.5_medium | address_books | 5.33 | 20.50 | 99,453 |
| claude_sonnet_4.5_medium | bibliography | 6.74 | 13.20 | 64,693 |
| claude_sonnet_4.5_medium | military_records | 26.50 | 38.19 | 2,898 |
| gemini_3.0_flash | address_books | 6.49 | 20.45 | 99,453 |
| gemini_3.0_flash | bibliography | 6.51 | 11.39 | 64,693 |
| gemini_3.0_flash | military_records | 38.13 | 51.01 | 2,898 |
| gemini_3.0_pro_medium | address_books | 1.97 | 16.30 | 99,453 |
| gemini_3.0_pro_medium | bibliography | 6.71 | 13.12 | 64,693 |
| gemini_3.0_pro_medium | military_records | 86.20 | 103.77 | 2,898 |
| gpt_5.2_medium | address_books | 5.53 | 29.01 | 99,453 |
| gpt_5.2_medium | bibliography | 6.15 | 9.19 | 64,693 |
| gpt_5.2_medium | military_records | 24.91 | 32.91 | 2,898 |
| gpt_5_mini_medium | address_books | 5.76 | 33.76 | 99,453 |
| gpt_5_mini_medium | bibliography | 41.60 | 46.05 | 64,693 |
| gpt_5_mini_medium | military_records | 24.43 | 41.71 | 2,898 |
| tesseract | address_books | 87.87 | 145.18 | 99,453 |
| tesseract | bibliography | 9.53 | 19.34 | 64,693 |
| tesseract | military_records | 71.43 | 138.19 | 2,898 |

### Formatting Compliance

| Model | Category | Page Tag Recall (%) | Heading Recall (%) | Bold/Italic Ratio (%) |
|-------|----------|---------------------|--------------------|-----------------------|
| claude_haiku_4.5_medium | address_books | 38.7 | N/A | 22.8 |
| claude_haiku_4.5_medium | bibliography | 76.7 | N/A | 31.0 |
| claude_haiku_4.5_medium | military_records | 0.0 | 100.0 | 5500.0 |
| claude_sonnet_4.5_medium | address_books | 32.3 | N/A | 28.9 |
| claude_sonnet_4.5_medium | bibliography | 90.0 | N/A | 51.7 |
| claude_sonnet_4.5_medium | military_records | 100.0 | 100.0 | 2100.0 |
| gemini_3.0_flash | address_books | 93.5 | N/A | 74.6 |
| gemini_3.0_flash | bibliography | 100.0 | N/A | 113.8 |
| gemini_3.0_flash | military_records | 300.0 | 100.0 | 3400.0 |
| gemini_3.0_pro_medium | address_books | 96.8 | N/A | 95.3 |
| gemini_3.0_pro_medium | bibliography | 93.3 | N/A | 63.8 |
| gemini_3.0_pro_medium | military_records | 0.0 | 150.0 | 200.0 |
| gpt_5.2_medium | address_books | 3.2 | N/A | 0.6 |
| gpt_5.2_medium | bibliography | 100.0 | N/A | 5.2 |
| gpt_5.2_medium | military_records | 100.0 | 100.0 | 0.0 |
| gpt_5_mini_medium | address_books | 22.6 | N/A | 0.3 |
| gpt_5_mini_medium | bibliography | 60.0 | N/A | 24.1 |
| gpt_5_mini_medium | military_records | 100.0 | 150.0 | 0.0 |
| tesseract | address_books | 0.0 | N/A | 5.3 |
| tesseract | bibliography | 0.0 | N/A | 3.4 |
| tesseract | military_records | 0.0 | 0.0 | 100.0 |

## Overall Rankings (Pure CER)

| Rank | Model | CER (%) | WER (%) |
|------|-------|---------|--------|
| 1 | gemini_3.0_pro_medium | 5.27 | 16.26 |
| 2 | gpt_5.2_medium | 6.11 | 19.89 |
| 3 | claude_sonnet_4.5_medium | 6.24 | 17.41 |
| 4 | gemini_3.0_flash | 7.05 | 16.75 |
| 5 | claude_haiku_4.5_medium | 8.09 | 25.82 |
| 6 | gpt_5_mini_medium | 19.96 | 39.59 |
| 7 | tesseract | 57.24 | 86.78 |
