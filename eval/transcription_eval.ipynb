{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoTranscriber Evaluation: CER & WER Analysis\n",
    "\n",
    "This notebook evaluates transcription quality across multiple transcription systems (local OCR and multiple LLM providers) using edit-distance–based accuracy metrics computed against manually corrected ground truth.\n",
    "\n",
    "The primary evaluation outputs are:\n",
    "- **Character Error Rate (CER)**: edit distance at the character level, divided by the number of reference characters.\n",
    "- **Word Error Rate (WER)**: edit distance at the word level, divided by the number of reference words.\n",
    "\n",
    "## Evaluation Method\n",
    "The evaluation is performed **page-by-page** using the temporary JSONL files produced by the ChronoTranscriber pipeline. Page-level evaluation is preferred over comparing final exported plain-text files because it:\n",
    "- Avoids penalizing downstream whitespace or post-processing differences that do not reflect transcription quality.\n",
    "- Preserves page boundaries to support error localization and qualitative inspection.\n",
    "- Enables fairer comparisons across models when outputs vary in formatting.\n",
    "\n",
    "The metrics implementation distinguishes:\n",
    "- **Overall metrics**: computed on the full text (including markup and page markers if present).\n",
    "- **Content-only metrics**: computed after removing formatting artifacts to focus on raw text recognition.\n",
    "- **Formatting metrics** (available in the underlying metric implementation): separate accounting for page markers and common Markdown constructs.\n",
    "\n",
    "## Models Evaluated\n",
    "| Provider | Model | `model_id` | Reasoning |\n",
    "|----------|-------|-----------|----------|\n",
    "| Local | Tesseract OCR | `tesseract` | None (baseline) |\n",
    "| OpenAI | GPT-5.2 | `gpt-5.2` | Medium |\n",
    "| OpenAI | GPT-5 Mini | `gpt-5-mini` | Medium |\n",
    "| Google | Gemini 3 Pro | `gemini-3-pro` | Medium |\n",
    "| Google | Gemini 3 Flash | `gemini-3-flash-preview` | None |\n",
    "| Anthropic | Claude Sonnet 4.5 | `claude-sonnet-4-5-20250929` | Medium |\n",
    "| Anthropic | Claude Haiku 4.5 | `claude-haiku-4-5` | Medium |\n",
    "\n",
    "## Dataset Categories\n",
    "The evaluation dataset is organized into three document categories defined in the configuration:\n",
    "1. **Address Books** — Swiss address book pages (Basel 1900); 31 pages processed as one source\n",
    "2. **Bibliography** — European culinary bibliography (Oxford 1913); 187 pages\n",
    "3. **Military Records** — Brazilian military enlistment cards; 3 sources × 2 pages each\n",
    "\n",
    "## Ground Truth\n",
    "Manually corrected reference transcriptions are stored as JSONL in `test_data/ground_truth/`\n",
    "(converted from `Korrekturen.zip` via `setup_ground_truth.py`). Schema normalizations applied:\n",
    "- Image tags: `[Image: ...]` → `![Image: ...]`\n",
    "- Page markers: `<page_number>X<page_number>` → `<page_number>X</page_number>`\n",
    "\n",
    "## Reproducibility\n",
    "This notebook is designed to be paper-ready and reproducible:\n",
    "- Key result tables are rendered inline as HTML for stable visual inspection in the notebook.\n",
    "- If `SAVE_TABLES_LATEX = True`, tables are also exported as TeX table files to `LATEX_OUTPUT_DIR` with captions and labels suitable for manuscript inclusion.\n",
    "- Run metadata (timestamp and output locations) are printed in the Setup section for provenance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. **Configuration**\n",
    "   Load the evaluation configuration (YAML), resolve all dataset/report paths, and document the run-time settings used for this evaluation.\n",
    "\n",
    "2. **Discover Available Data**\n",
    "   Enumerate available input sources, model output JSONL files, and ground-truth JSONL files to determine which model–category combinations can be evaluated.\n",
    "\n",
    "3. **Page-Level Evaluation**\n",
    "   Define the evaluation data structures and compute page-aligned CER/WER metrics by comparing each model’s JSONL transcription output against ground truth.\n",
    "\n",
    "4. **Results Summary**\n",
    "   Produce paper-ready summary tables (inline HTML; optional LaTeX export) showing model performance by category and overall rankings across categories.\n",
    "\n",
    "5. **Detailed Per-Page Results**\n",
    "   Display (and optionally export) a per-page table for a selected `SHOW_CATEGORY` / `SHOW_MODEL` / `SHOW_SOURCE` to support qualitative inspection and debugging.\n",
    "\n",
    "6. **Export Results**\n",
    "   Save machine-readable artifacts (JSON + CSV + Markdown) into the reports directory for archiving and downstream analysis; show an export summary table.\n",
    "\n",
    "7. **Visualization (Optional)**\n",
    "   Generate a compact figure comparing models on CER and WER; save the plot for figures and optionally as PDF for LaTeX workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Standard library imports\n",
    "# =============================================================================\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# Data handling and display\n",
    "# =============================================================================\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# =============================================================================\n",
    "# Path Configuration\n",
    "# =============================================================================\n",
    "EVAL_DIR = Path.cwd()\n",
    "PROJECT_ROOT = EVAL_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT CONFIGURATION\n",
    "# =============================================================================\n",
    "# Set to True to save tables as LaTeX .tex files\n",
    "SAVE_TABLES_LATEX = True\n",
    "\n",
    "# Directory for LaTeX table output (relative to EVAL_DIR)\n",
    "LATEX_OUTPUT_DIR = EVAL_DIR / \"reports\" / \"latex_tables\"\n",
    "\n",
    "# Create output directory if saving LaTeX\n",
    "if SAVE_TABLES_LATEX:\n",
    "    LATEX_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# Import evaluation metrics\n",
    "# =============================================================================\n",
    "from metrics import (\n",
    "    compute_metrics,\n",
    "    aggregate_metrics,\n",
    "    TranscriptionMetrics,\n",
    "    format_metrics_table,\n",
    ")\n",
    "\n",
    "# Import JSONL page-level utilities\n",
    "from jsonl_eval import (\n",
    "    PageTranscription,\n",
    "    DocumentTranscriptions,\n",
    "    parse_transcription_jsonl,\n",
    "    find_jsonl_file,\n",
    "    load_page_transcriptions,\n",
    "    load_ground_truth_pages,\n",
    "    align_pages,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Run Summary\n",
    "# =============================================================================\n",
    "print(f\"Analysis run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Save tables as LaTeX: {SAVE_TABLES_LATEX}\")\n",
    "if SAVE_TABLES_LATEX:\n",
    "    print(f\"LaTeX output directory: {LATEX_OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "This section loads the evaluation configuration (YAML) and resolves all key paths used throughout the notebook. The configuration defines:\n",
    "\n",
    "- **Dataset locations**\n",
    "  - `INPUT_PATH`: source documents (images or PDFs), grouped by category.\n",
    "  - `OUTPUT_PATH`: model-generated JSONL transcriptions (the hypothesis).\n",
    "  - `GROUND_TRUTH_PATH`: manually corrected JSONL transcriptions (the reference).\n",
    "  - `REPORTS_PATH`: where exported evaluation artifacts are written.\n",
    "\n",
    "- **Evaluation scope**\n",
    "  - `CATEGORIES`: which dataset categories are included.\n",
    "  - `MODELS`: which model identifiers are available or expected.\n",
    "\n",
    "**Output produced in this section**\n",
    "- **Table 1**: a compact configuration table documenting the paths and scope used for the current run.\n",
    "- Optional: a TeX export of Table 1 saved to `LATEX_OUTPUT_DIR` if `SAVE_TABLES_LATEX = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load evaluation configuration\n",
    "# =============================================================================\n",
    "CONFIG_PATH = EVAL_DIR / \"eval_config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "INPUT_PATH = EVAL_DIR / config['dataset']['input_path']\n",
    "OUTPUT_PATH = EVAL_DIR / config['dataset']['output_path']\n",
    "GROUND_TRUTH_PATH = EVAL_DIR / config['dataset']['ground_truth_path']\n",
    "REPORTS_PATH = EVAL_DIR / config['evaluation']['reports_path']\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract categories and models\n",
    "CATEGORIES = [cat['name'] for cat in config['dataset']['categories']]\n",
    "MODELS = {m['name']: m for m in config['models']}\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration Summary Table\n",
    "# =============================================================================\n",
    "config_data = {\n",
    "    'Parameter': ['Input Path', 'Output Path', 'Ground Truth Path', 'Reports Path',\n",
    "                  'Categories', 'Models'],\n",
    "    'Value': [str(INPUT_PATH), str(OUTPUT_PATH), str(GROUND_TRUTH_PATH), str(REPORTS_PATH),\n",
    "              ', '.join(CATEGORIES), str(len(MODELS))]\n",
    "}\n",
    "df_config = pd.DataFrame(config_data)\n",
    "\n",
    "display(HTML('<h4>Table 1: Evaluation Configuration</h4>'))\n",
    "display(HTML(df_config.to_html(index=False)))\n",
    "\n",
    "if SAVE_TABLES_LATEX:\n",
    "    latex_path = LATEX_OUTPUT_DIR / 'table_01_configuration.tex'\n",
    "    df_config.to_latex(latex_path, index=False,\n",
    "                       caption='Evaluation Configuration Parameters',\n",
    "                       label='tab:eval_config')\n",
    "    print(f'Saved: {latex_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discover Available Data\n",
    "\n",
    "This section audits the evaluation data on disk to determine what can be evaluated in the current run.\n",
    "\n",
    "Using the configured paths, it:\n",
    "- Lists available **input sources** for each category.\n",
    "- Detects which **models have produced JSONL output** for each category.\n",
    "- Checks whether **ground truth JSONL files** exist for each category.\n",
    "\n",
    "**Why this matters**\n",
    "- The evaluation proceeds only for category–model combinations where both hypothesis output and ground truth exist.\n",
    "- This step makes missing artifacts explicit before running more expensive computations.\n",
    "\n",
    "**Output produced in this section**\n",
    "- A console summary for each category showing:\n",
    "  - Number of detected sources.\n",
    "  - Which models have JSONL output.\n",
    "  - Whether ground truth exists (and how many files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sources(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover source files/folders in the input directory for a category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of source names\n",
    "    \"\"\"\n",
    "    input_dir = INPUT_PATH / category\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    sources = []\n",
    "    has_direct_images = False\n",
    "    \n",
    "    for item in input_dir.iterdir():\n",
    "        if item.is_file() and item.suffix.lower() == '.pdf':\n",
    "            sources.append(item.name)\n",
    "        elif item.is_file() and item.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tiff']:\n",
    "            has_direct_images = True\n",
    "        elif item.is_dir():\n",
    "            # Check if folder contains images\n",
    "            images = list(item.glob('*.jpg')) + list(item.glob('*.png'))\n",
    "            if images:\n",
    "                sources.append(item.name)\n",
    "    \n",
    "    # If images are directly in the category folder (not in subfolders),\n",
    "    # treat the whole folder as a single source named after the category.\n",
    "    if has_direct_images and not sources:\n",
    "        sources.append(category)\n",
    "    \n",
    "    return sorted(sources)\n",
    "\n",
    "\n",
    "def discover_available_models(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover which models have JSONL output for a given category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of model names with available output\n",
    "    \"\"\"\n",
    "    output_dir = OUTPUT_PATH / category\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    models = []\n",
    "    for d in output_dir.iterdir():\n",
    "        if d.is_dir():\n",
    "            # Check if model directory has any JSONL files\n",
    "            jsonl_files = list(d.rglob('*.jsonl'))\n",
    "            if jsonl_files:\n",
    "                models.append(d.name)\n",
    "    \n",
    "    return sorted(models)\n",
    "\n",
    "\n",
    "def check_ground_truth_available(category: str) -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Check if ground truth JSONL files exist for a category.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (has_ground_truth, count_of_files)\n",
    "    \"\"\"\n",
    "    gt_dir = GROUND_TRUTH_PATH / category\n",
    "    if not gt_dir.exists():\n",
    "        return False, 0\n",
    "    \n",
    "    jsonl_files = list(gt_dir.glob('*.jsonl'))\n",
    "    return len(jsonl_files) > 0, len(jsonl_files)\n",
    "\n",
    "\n",
    "# Discover and display available data\n",
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_summary = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    sources = discover_sources(category)\n",
    "    available_models = discover_available_models(category)\n",
    "    gt_available, gt_count = check_ground_truth_available(category)\n",
    "    \n",
    "    data_summary[category] = {\n",
    "        'sources': sources,\n",
    "        'models': available_models,\n",
    "        'ground_truth_available': gt_available,\n",
    "        'ground_truth_count': gt_count,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Input sources: {len(sources)}\")\n",
    "    if sources:\n",
    "        for s in sources[:5]:\n",
    "            print(f\"    - {s}\")\n",
    "        if len(sources) > 5:\n",
    "            print(f\"    ... and {len(sources) - 5} more\")\n",
    "    print(f\"  Models with JSONL output: {len(available_models)}\")\n",
    "    for m in available_models:\n",
    "        print(f\"    - {m}\")\n",
    "    print(f\"  Ground truth JSONL: {'Yes' if gt_available else 'No'} ({gt_count} files)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Page-Level Evaluation\n",
    "\n",
    "This section defines the evaluation logic and computes accuracy metrics at the **page** level.\n",
    "\n",
    "### Unit of analysis\n",
    "The unit of analysis is a page transcription parsed from a JSONL file. Pages are aligned between:\n",
    "- **Reference**: ground truth JSONL\n",
    "- **Hypothesis**: model output JSONL\n",
    "\n",
    "A page is evaluated only if:\n",
    "- The ground-truth page exists and contains transcribable text, and\n",
    "- The model output page exists and contains transcribable text.\n",
    "\n",
    "Pages flagged as `no_transcribable_text` or `transcription_not_possible` are treated as non-evaluable.\n",
    "\n",
    "### What is computed\n",
    "For each aligned, evaluable page, the notebook calls .compute_metrics(...) to produce:\n",
    "- Overall CER and WER\n",
    "- Content-only CER and WER (formatting stripped)\n",
    "\n",
    "Page-level results are then aggregated with .aggregate_metrics(...) (micro-averaging by reference length).\n",
    "\n",
    "**Output produced in this section**\n",
    "- In-memory structures:\n",
    "  - `all_results`: nested results by category, model, source, and page.\n",
    "  - `aggregated_metrics`: summary metrics by category and model.\n",
    "- A console progress summary for each evaluated model/category showing:\n",
    "  - CER and WER\n",
    "  - Count of evaluated pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageEvaluationResult:\n",
    "    \"\"\"Container for per-page evaluation results.\"\"\"\n",
    "    page_index: int\n",
    "    image_name: str\n",
    "    metrics: Optional[TranscriptionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SourceEvaluationResult:\n",
    "    \"\"\"Container for source-level evaluation results.\"\"\"\n",
    "    category: str\n",
    "    model_name: str\n",
    "    source_name: str\n",
    "    page_results: List[PageEvaluationResult]\n",
    "    aggregated_metrics: Optional[TranscriptionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def total_pages(self) -> int:\n",
    "        return len(self.page_results)\n",
    "    \n",
    "    @property\n",
    "    def evaluated_pages(self) -> int:\n",
    "        return sum(1 for p in self.page_results if p.metrics is not None)\n",
    "\n",
    "\n",
    "def evaluate_source_pages(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    "    source_name: str,\n",
    ") -> SourceEvaluationResult:\n",
    "    \"\"\"\n",
    "    Evaluate a source by comparing pages from model output to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        SourceEvaluationResult with per-page and aggregated metrics\n",
    "    \"\"\"\n",
    "    # Load ground truth pages\n",
    "    gt_doc = load_ground_truth_pages(GROUND_TRUTH_PATH, category, source_name)\n",
    "    if gt_doc is None or not gt_doc.pages:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            page_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=False,\n",
    "            output_found=False,\n",
    "            error=\"Ground truth JSONL not found\",\n",
    "        )\n",
    "    \n",
    "    # Load model output pages\n",
    "    hyp_doc = load_page_transcriptions(OUTPUT_PATH, category, model_name, source_name)\n",
    "    if hyp_doc is None or not hyp_doc.pages:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            page_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=False,\n",
    "            error=\"Model output JSONL not found\",\n",
    "        )\n",
    "    \n",
    "    # Align pages\n",
    "    aligned = align_pages(hyp_doc, gt_doc)\n",
    "    \n",
    "    # Compute per-page metrics\n",
    "    page_results: List[PageEvaluationResult] = []\n",
    "    valid_metrics: List[TranscriptionMetrics] = []\n",
    "    \n",
    "    for hyp_page, gt_page in aligned:\n",
    "        # Determine page info\n",
    "        if gt_page:\n",
    "            page_index = gt_page.page_index\n",
    "            image_name = gt_page.image_name or (hyp_page.image_name if hyp_page else \"\")\n",
    "        elif hyp_page:\n",
    "            page_index = hyp_page.page_index\n",
    "            image_name = hyp_page.image_name\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check availability\n",
    "        gt_found = gt_page is not None and gt_page.has_text()\n",
    "        hyp_found = hyp_page is not None and hyp_page.has_text()\n",
    "        \n",
    "        if not gt_found:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=False,\n",
    "                output_found=hyp_found,\n",
    "                error=\"No ground truth for page\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        if not hyp_found:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=False,\n",
    "                error=\"No model output for page\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            metrics = compute_metrics(\n",
    "                gt_page.transcription,\n",
    "                hyp_page.transcription,\n",
    "                normalize=True,\n",
    "            )\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=metrics,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "            ))\n",
    "            valid_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                error=str(e),\n",
    "            ))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    aggregated = aggregate_metrics(valid_metrics) if valid_metrics else None\n",
    "    \n",
    "    return SourceEvaluationResult(\n",
    "        category=category,\n",
    "        model_name=model_name,\n",
    "        source_name=source_name,\n",
    "        page_results=page_results,\n",
    "        aggregated_metrics=aggregated,\n",
    "        ground_truth_found=True,\n",
    "        output_found=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model_category(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    ") -> Tuple[List[SourceEvaluationResult], Optional[TranscriptionMetrics]]:\n",
    "    \"\"\"\n",
    "    Evaluate all sources in a category for a given model.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of per-source results, aggregated metrics)\n",
    "    \"\"\"\n",
    "    sources = discover_sources(category)\n",
    "    results = []\n",
    "    all_page_metrics = []\n",
    "    \n",
    "    for source in sources:\n",
    "        result = evaluate_source_pages(category, model_name, source)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Collect valid page metrics for aggregation\n",
    "        for page_result in result.page_results:\n",
    "            if page_result.metrics is not None:\n",
    "                all_page_metrics.append(page_result.metrics)\n",
    "    \n",
    "    aggregated = aggregate_metrics(all_page_metrics) if all_page_metrics else None\n",
    "    \n",
    "    return results, aggregated\n",
    "\n",
    "\n",
    "print(\"Page-level evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING PAGE-LEVEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results: Dict[str, Dict[str, List[SourceEvaluationResult]]] = {}\n",
    "aggregated_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    all_results[category] = {}\n",
    "    aggregated_metrics[category] = {}\n",
    "    \n",
    "    available_models = discover_available_models(category)\n",
    "    gt_available, gt_count = check_ground_truth_available(category)\n",
    "    \n",
    "    if not available_models:\n",
    "        print(f\"\\n{category}: No model outputs found (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    if not gt_available:\n",
    "        print(f\"\\n{category}: No ground truth JSONL files (skipping)\")\n",
    "        print(f\"  Hint: Run 'python main/prepare_ground_truth.py --extract' to create editable files\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in available_models:\n",
    "        results, agg_metrics = evaluate_model_category(category, model_name)\n",
    "        all_results[category][model_name] = results\n",
    "        \n",
    "        if agg_metrics:\n",
    "            aggregated_metrics[category][model_name] = agg_metrics\n",
    "            total_pages = sum(r.total_pages for r in results)\n",
    "            eval_pages = sum(r.evaluated_pages for r in results)\n",
    "            print(f\"  {model_name}:\")\n",
    "            print(f\"    CER: {agg_metrics.cer*100:.2f}%  |  WER: {agg_metrics.wer*100:.2f}%\")\n",
    "            print(f\"    Pages evaluated: {eval_pages}/{total_pages}\")\n",
    "        else:\n",
    "            errors = [r.error for r in results if r.error]\n",
    "            print(f\"  {model_name}: No valid evaluations\")\n",
    "            if errors:\n",
    "                print(f\"    Error: {errors[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary\n",
    "\n",
    "This section converts the evaluation results into publication-ready summary tables.\n",
    "\n",
    "### What is summarized\n",
    "- **By category**: performance for each model within each dataset category (`CATEGORIES`).\n",
    "- **Overall**: performance for each model aggregated across all categories.\n",
    "\n",
    "### Outputs produced\n",
    "- **Table 2**: Transcription accuracy by model and category (inline HTML; optional LaTeX export).\n",
    "- **Table 3**: Overall model ranking across categories (inline HTML; optional LaTeX export).\n",
    "\n",
    "If `SAVE_TABLES_LATEX = True`, the corresponding TeX table files are written to `LATEX_OUTPUT_DIR` using `DataFrame.to_latex(...)` with captions and labels suitable for manuscript inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. Results Summary - Restructure for display\n",
    "# =============================================================================\n",
    "\n",
    "# Restructure for display: model -> category -> metrics\n",
    "model_category_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category, models in aggregated_metrics.items():\n",
    "    for model_name, metrics in models.items():\n",
    "        if model_name not in model_category_metrics:\n",
    "            model_category_metrics[model_name] = {}\n",
    "        model_category_metrics[model_name][category] = metrics\n",
    "\n",
    "# =============================================================================\n",
    "# Build Results DataFrame for HTML/LaTeX output\n",
    "# =============================================================================\n",
    "if model_category_metrics:\n",
    "    results_rows = []\n",
    "    for model_name in sorted(model_category_metrics.keys()):\n",
    "        for category in CATEGORIES:\n",
    "            if category in model_category_metrics[model_name]:\n",
    "                m = model_category_metrics[model_name][category]\n",
    "                results_rows.append({\n",
    "                    'Model': model_name,\n",
    "                    'Category': category,\n",
    "                    'CER (%)': f'{m.cer*100:.2f}',\n",
    "                    'WER (%)': f'{m.wer*100:.2f}',\n",
    "                    'Content CER (%)': f'{m.content_cer*100:.2f}',\n",
    "                    'Content WER (%)': f'{m.content_wer*100:.2f}',\n",
    "                    'Ref. Characters': f'{m.ref_char_count:,}',\n",
    "                    'Ref. Words': f'{m.ref_word_count:,}',\n",
    "                })\n",
    "\n",
    "    df_results = pd.DataFrame(results_rows)\n",
    "\n",
    "    display(HTML('<h4>Table 2: Transcription Accuracy by Model and Category</h4>'))\n",
    "    display(HTML(df_results.to_html(index=False)))\n",
    "\n",
    "    if SAVE_TABLES_LATEX:\n",
    "        latex_path = LATEX_OUTPUT_DIR / 'table_02_results_by_category.tex'\n",
    "        df_results.to_latex(latex_path, index=False,\n",
    "                            caption='Transcription Accuracy Metrics by Model and Document Category',\n",
    "                            label='tab:results_by_category')\n",
    "        print(f'Saved: {latex_path}')\n",
    "else:\n",
    "    print(\"\\nNo evaluation results available.\")\n",
    "    print(\"\\nTo generate results:\")\n",
    "    print(\"1. Run transcriptions for each model (outputs go to test_data/output/{category}/{model_name}/)\")\n",
    "    print(\"2. Create ground truth JSONL files:\")\n",
    "    print(\"   a. python main/prepare_ground_truth.py --extract --input test_data/output/{category}/{model}\")\n",
    "    print(\"   b. Edit the generated _editable.txt files\")\n",
    "    print(\"   c. python main/prepare_ground_truth.py --apply --input {edited_file}\")\n",
    "    print(\"3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Overall Model Performance (All Categories Combined)\n",
    "# =============================================================================\n",
    "\n",
    "overall_model_metrics = {}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    all_metrics = list(cat_metrics.values())\n",
    "    if all_metrics:\n",
    "        overall = aggregate_metrics(all_metrics)\n",
    "        overall_model_metrics[model_name] = overall\n",
    "\n",
    "# Build ranking DataFrame\n",
    "if overall_model_metrics:\n",
    "    ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "\n",
    "    ranking_rows = []\n",
    "    for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "        ranking_rows.append({\n",
    "            'Rank': rank,\n",
    "            'Model': model_name,\n",
    "            'CER (%)': f'{metrics.cer*100:.2f}',\n",
    "            'WER (%)': f'{metrics.wer*100:.2f}',\n",
    "            'Content CER (%)': f'{metrics.content_cer*100:.2f}',\n",
    "            'Content WER (%)': f'{metrics.content_wer*100:.2f}',\n",
    "            'Total Characters': f'{metrics.ref_char_count:,}',\n",
    "            'Total Words': f'{metrics.ref_word_count:,}',\n",
    "        })\n",
    "\n",
    "    df_ranking = pd.DataFrame(ranking_rows)\n",
    "\n",
    "    display(HTML('<h4>Table 3: Overall Model Rankings (All Categories Combined)</h4>'))\n",
    "    display(HTML(df_ranking.to_html(index=False)))\n",
    "\n",
    "    if SAVE_TABLES_LATEX:\n",
    "        latex_path = LATEX_OUTPUT_DIR / 'table_03_overall_rankings.tex'\n",
    "        df_ranking.to_latex(latex_path, index=False,\n",
    "                            caption='Overall Model Rankings by Character Error Rate (All Categories Combined)',\n",
    "                            label='tab:overall_rankings')\n",
    "        print(f'Saved: {latex_path}')\n",
    "else:\n",
    "    print(\"No overall metrics available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Per-Page Results\n",
    "\n",
    "This section supports qualitative inspection by drilling down from aggregate metrics to individual pages.\n",
    "\n",
    "### How to use this section\n",
    "The code cell below is parameterized by:\n",
    "- `SHOW_CATEGORY`: which dataset category to inspect.\n",
    "- `SHOW_MODEL`: which model to inspect (or `None` to use the first available).\n",
    "- `SHOW_SOURCE`: which source document to inspect (or `None` to use the first available).\n",
    "\n",
    "### Output produced\n",
    "- **Table 4**: Per-page CER/WER (and status codes for missing or unevaluable pages) for the selected model/source.\n",
    "- Optional: a TeX export of the same per-page table to `LATEX_OUTPUT_DIR`.\n",
    "\n",
    "This table is intended primarily for:\n",
    "- Diagnosing systematic failure modes (layout, scripts, tables, degraded scans).\n",
    "- Identifying outlier pages that dominate aggregate error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. Detailed Per-Page Results\n",
    "# =============================================================================\n",
    "\n",
    "# Configurable: select category/model/source to display\n",
    "SHOW_CATEGORY = \"address_books\"  # Change as needed\n",
    "SHOW_MODEL = None  # Set to specific model name or None for first available\n",
    "SHOW_SOURCE = None  # Set to specific source name or None for first available\n",
    "\n",
    "if SHOW_CATEGORY in all_results and all_results[SHOW_CATEGORY]:\n",
    "    print(f\"Detailed Page-Level Results: {SHOW_CATEGORY.upper()}\")\n",
    "\n",
    "    models_to_show = [SHOW_MODEL] if SHOW_MODEL else list(all_results[SHOW_CATEGORY].keys())[:1]\n",
    "\n",
    "    for model_name in models_to_show:\n",
    "        if model_name not in all_results[SHOW_CATEGORY]:\n",
    "            continue\n",
    "\n",
    "        results = all_results[SHOW_CATEGORY][model_name]\n",
    "        sources_to_show = [r for r in results if r.source_name == SHOW_SOURCE] if SHOW_SOURCE else results[:1]\n",
    "\n",
    "        for source_result in sources_to_show:\n",
    "            # Build per-page DataFrame\n",
    "            page_rows = []\n",
    "            for page_result in source_result.page_results:\n",
    "                page_num = page_result.page_index + 1\n",
    "                img_name = page_result.image_name[:40] + '...' if len(page_result.image_name) > 40 else page_result.image_name\n",
    "\n",
    "                if page_result.metrics:\n",
    "                    page_rows.append({\n",
    "                        'Page': page_num,\n",
    "                        'Image': img_name,\n",
    "                        'CER (%)': f'{page_result.metrics.cer*100:.2f}',\n",
    "                        'WER (%)': f'{page_result.metrics.wer*100:.2f}',\n",
    "                        'Status': 'OK',\n",
    "                    })\n",
    "                else:\n",
    "                    page_rows.append({\n",
    "                        'Page': page_num,\n",
    "                        'Image': img_name,\n",
    "                        'CER (%)': '--',\n",
    "                        'WER (%)': '--',\n",
    "                        'Status': page_result.error or 'Error',\n",
    "                    })\n",
    "\n",
    "            # Add totals row if aggregated metrics exist\n",
    "            if source_result.aggregated_metrics:\n",
    "                m = source_result.aggregated_metrics\n",
    "                page_rows.append({\n",
    "                    'Page': 'TOTAL',\n",
    "                    'Image': '',\n",
    "                    'CER (%)': f'{m.cer*100:.2f}',\n",
    "                    'WER (%)': f'{m.wer*100:.2f}',\n",
    "                    'Status': '',\n",
    "                })\n",
    "\n",
    "            df_pages = pd.DataFrame(page_rows)\n",
    "\n",
    "            display(HTML(f'<h4>Table 4: Per-Page Results - {model_name} / {source_result.source_name}</h4>'))\n",
    "            display(HTML(df_pages.to_html(index=False)))\n",
    "\n",
    "            if SAVE_TABLES_LATEX:\n",
    "                # Sanitize filename\n",
    "                safe_model = model_name.replace('.', '_').replace(' ', '_')\n",
    "                safe_source = source_result.source_name.replace('.', '_').replace(' ', '_')[:30]\n",
    "                latex_path = LATEX_OUTPUT_DIR / f'table_04_pages_{safe_model}_{safe_source}.tex'\n",
    "                df_pages.to_latex(latex_path, index=False,\n",
    "                                  caption=f'Per-Page Transcription Results: {model_name}, {source_result.source_name}',\n",
    "                                  label=f'tab:pages_{safe_model}_{safe_source}')\n",
    "                print(f'Saved: {latex_path}')\n",
    "else:\n",
    "    print(f\"Category '{SHOW_CATEGORY}' not found in results or has no evaluated models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "This section exports evaluation outputs to disk for reproducibility, archiving, and downstream analysis.\n",
    "\n",
    "### Files written\n",
    "- A timestamped JSON report containing structured metrics suitable for programmatic reuse.\n",
    "- A timestamped CSV file containing a row per page-level result for spreadsheet workflows.\n",
    "- A timestamped Markdown report containing a human-readable summary (including a Markdown-formatted table).\n",
    "\n",
    "### Output produced\n",
    "- **Table 5**: Export summary (paths and record counts), rendered inline as HTML.\n",
    "- Optional: a TeX export of the export summary table saved to `LATEX_OUTPUT_DIR`.\n",
    "\n",
    "All export paths are anchored to `REPORTS_PATH` (for JSON/CSV/Markdown) and `LATEX_OUTPUT_DIR` (for LaTeX artifacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. Export Results\n",
    "# =============================================================================\n",
    "import csv\n",
    "\n",
    "# Generate timestamp for reports\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# =============================================================================\n",
    "# Export aggregated metrics to JSON\n",
    "# =============================================================================\n",
    "json_report = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"evaluation_method\": \"page_level_jsonl\",\n",
    "    \"categories\": CATEGORIES,\n",
    "    \"models\": list(MODELS.keys()),\n",
    "    \"results\": {},\n",
    "}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    json_report[\"results\"][model_name] = {\n",
    "        \"per_category\": {cat: m.to_dict() for cat, m in cat_metrics.items()},\n",
    "    }\n",
    "    if model_name in overall_model_metrics:\n",
    "        json_report[\"results\"][model_name][\"overall\"] = overall_model_metrics[model_name].to_dict()\n",
    "\n",
    "json_path = REPORTS_PATH / f\"eval_results_{timestamp}.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_report, f, indent=2)\n",
    "print(f\"JSON report saved: {json_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Export to CSV (per-page detail)\n",
    "# =============================================================================\n",
    "csv_rows = []\n",
    "for category, models in all_results.items():\n",
    "    for model_name, sources in models.items():\n",
    "        for source_result in sources:\n",
    "            for page_result in source_result.page_results:\n",
    "                if page_result.metrics:\n",
    "                    csv_rows.append({\n",
    "                        'Model': model_name,\n",
    "                        'Category': category,\n",
    "                        'Source': source_result.source_name,\n",
    "                        'Page': page_result.page_index + 1,\n",
    "                        'Image': page_result.image_name,\n",
    "                        'CER (%)': round(page_result.metrics.cer * 100, 2),\n",
    "                        'WER (%)': round(page_result.metrics.wer * 100, 2),\n",
    "                        'Char Distance': page_result.metrics.char_distance,\n",
    "                        'Ref Chars': page_result.metrics.ref_char_count,\n",
    "                        'Status': 'OK',\n",
    "                    })\n",
    "                else:\n",
    "                    csv_rows.append({\n",
    "                        'Model': model_name,\n",
    "                        'Category': category,\n",
    "                        'Source': source_result.source_name,\n",
    "                        'Page': page_result.page_index + 1,\n",
    "                        'Image': page_result.image_name,\n",
    "                        'CER (%)': '',\n",
    "                        'WER (%)': '',\n",
    "                        'Char Distance': '',\n",
    "                        'Ref Chars': '',\n",
    "                        'Status': page_result.error or 'Error',\n",
    "                    })\n",
    "\n",
    "df_csv = pd.DataFrame(csv_rows)\n",
    "csv_path = REPORTS_PATH / f\"eval_results_{timestamp}.csv\"\n",
    "df_csv.to_csv(csv_path, index=False)\n",
    "print(f\"CSV report saved: {csv_path}\")\n",
    "\n",
    "# Display summary of exported data\n",
    "display(HTML('<h4>Table 5: Export Summary</h4>'))\n",
    "export_summary = pd.DataFrame({\n",
    "    'Export Type': ['JSON Report', 'CSV (Per-Page)', 'LaTeX Tables'],\n",
    "    'File Path': [str(json_path), str(csv_path),\n",
    "                  str(LATEX_OUTPUT_DIR) if SAVE_TABLES_LATEX else 'Disabled'],\n",
    "    'Records': [len(json_report['results']), len(csv_rows),\n",
    "                len(list(LATEX_OUTPUT_DIR.glob('*.tex'))) if SAVE_TABLES_LATEX else 0],\n",
    "})\n",
    "display(HTML(export_summary.to_html(index=False)))\n",
    "\n",
    "if SAVE_TABLES_LATEX:\n",
    "    latex_path = LATEX_OUTPUT_DIR / 'table_05_export_summary.tex'\n",
    "    export_summary.to_latex(latex_path, index=False,\n",
    "                            caption='Summary of Exported Evaluation Results',\n",
    "                            label='tab:export_summary')\n",
    "    print(f'Saved: {latex_path}')\n",
    "\n",
    "# =============================================================================\n",
    "# Export Markdown summary\n",
    "# =============================================================================\n",
    "md_path = REPORTS_PATH / f\"eval_results_{timestamp}.md\"\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# ChronoTranscriber Evaluation Results\\n\\n\")\n",
    "    f.write(f\"**Generated:** {timestamp}\\n\\n\")\n",
    "    f.write(f\"**Evaluation Method:** Page-level JSONL comparison\\n\\n\")\n",
    "    f.write(f\"## Models Evaluated\\n\\n\")\n",
    "    for name, info in MODELS.items():\n",
    "        f.write(f\"- **{name}**: {info.get('description', '')}\\n\")\n",
    "    f.write(f\"\\n## Results by Category\\n\\n\")\n",
    "    f.write(format_metrics_table(model_category_metrics, CATEGORIES))\n",
    "    f.write(f\"\\n\\n## Overall Rankings\\n\\n\")\n",
    "    if overall_model_metrics:\n",
    "        ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "        f.write(\"| Rank | Model | CER (%) | WER (%) |\\n\")\n",
    "        f.write(\"|------|-------|---------|--------|\\n\")\n",
    "        for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "            f.write(f\"| {rank} | {model_name} | {metrics.cer*100:.2f} | {metrics.wer*100:.2f} |\\n\")\n",
    "\n",
    "print(f\"Markdown report saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization (Optional)\n",
    "\n",
    "This section produces a compact figure comparing model error rates.\n",
    "\n",
    "### What is plotted\n",
    "If `matplotlib` is available and `overall_model_metrics` has data, the code:\n",
    "- Sorts models by overall CER for consistent ordering.\n",
    "- Plots horizontal bar charts for:\n",
    "  - CER (percent)\n",
    "  - WER (percent)\n",
    "\n",
    "### Files written\n",
    "- A timestamped PNG figure is saved to the reports directory for quick viewing and sharing.\n",
    "- If `SAVE_TABLES_LATEX = True`, a PDF version is also saved to `LATEX_OUTPUT_DIR` for LaTeX workflows.\n",
    "\n",
    "### Output produced\n",
    "- The figure is displayed inline in the notebook.\n",
    "- **Table 6**: A small performance summary table (sorted by CER) is displayed inline and optionally exported as TeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. Visualization (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    PLOT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOT_AVAILABLE = False\n",
    "    print(\"matplotlib not available - skipping visualizations\")\n",
    "    print(\"Install with: pip install matplotlib\")\n",
    "\n",
    "if PLOT_AVAILABLE and overall_model_metrics:\n",
    "    # Prepare data - sort by CER for consistent ordering\n",
    "    ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "    models = [m[0] for m in ranked]\n",
    "    cer_values = [m[1].cer * 100 for m in ranked]\n",
    "    wer_values = [m[1].wer * 100 for m in ranked]\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # CER bar chart\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.barh(models, cer_values, color='steelblue')\n",
    "    ax1.set_xlabel('Character Error Rate (%)')\n",
    "    ax1.set_title('CER by Model (Lower is Better)')\n",
    "    ax1.bar_label(bars1, fmt='%.2f%%', padding=3)\n",
    "    ax1.set_xlim(0, max(cer_values) * 1.25 if cer_values else 10)\n",
    "\n",
    "    # WER bar chart\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(models, wer_values, color='darkorange')\n",
    "    ax2.set_xlabel('Word Error Rate (%)')\n",
    "    ax2.set_title('WER by Model (Lower is Better)')\n",
    "    ax2.bar_label(bars2, fmt='%.2f%%', padding=3)\n",
    "    ax2.set_xlim(0, max(wer_values) * 1.25 if wer_values else 10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    fig_path = REPORTS_PATH / f\"eval_chart_{timestamp}.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Chart saved: {fig_path}\")\n",
    "\n",
    "    # Also save as PDF for LaTeX inclusion\n",
    "    if SAVE_TABLES_LATEX:\n",
    "        pdf_path = LATEX_OUTPUT_DIR / f\"figure_01_error_rates.pdf\"\n",
    "        plt.savefig(pdf_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"PDF chart saved: {pdf_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # =============================================================================\n",
    "    # Create summary statistics table for visualization\n",
    "    # =============================================================================\n",
    "    viz_stats = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'CER (%)': [f'{v:.2f}' for v in cer_values],\n",
    "        'WER (%)': [f'{v:.2f}' for v in wer_values],\n",
    "        'CER Rank': range(1, len(models) + 1),\n",
    "    })\n",
    "\n",
    "    display(HTML('<h4>Table 6: Model Performance Summary (Sorted by CER)</h4>'))\n",
    "    display(HTML(viz_stats.to_html(index=False)))\n",
    "\n",
    "    if SAVE_TABLES_LATEX:\n",
    "        latex_path = LATEX_OUTPUT_DIR / 'table_06_performance_summary.tex'\n",
    "        viz_stats.to_latex(latex_path, index=False,\n",
    "                           caption='Model Performance Summary Sorted by Character Error Rate',\n",
    "                           label='tab:performance_summary')\n",
    "        print(f'Saved: {latex_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Ground Truth Workflow\n",
    "\n",
    "1. **Extract transcriptions for editing**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --extract --input eval/test_data/output/{category}/{model}\n",
    "   ```\n",
    "\n",
    "2. **Edit the generated `_editable.txt` files**\n",
    "   - Each page is marked with `=== page NNN ===`\n",
    "   - Correct transcription errors directly in the text\n",
    "   - Use `[NO TRANSCRIBABLE TEXT]` for blank pages\n",
    "   - Use `[TRANSCRIPTION NOT POSSIBLE]` for illegible pages\n",
    "\n",
    "3. **Apply corrections to create ground truth**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --apply --input eval/test_data/output/{category}/{model}\n",
    "   ```\n",
    "\n",
    "4. **Check ground truth status**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --status\n",
    "   ```\n",
    "\n",
    "### Expected Directory Structure\n",
    "```\n",
    "eval/\n",
    "├── test_data/\n",
    "│   ├── input/                    # Source documents\n",
    "│   │   ├── address_books/\n",
    "│   │   ├── bibliography/\n",
    "│   │   └── military_records/\n",
    "│   ├── output/                   # Model outputs (JSONL per source)\n",
    "│   │   └── {category}/\n",
    "│   │       └── {model_name}/\n",
    "│   │           └── {source}/\n",
    "│   │               └── {source}.jsonl\n",
    "│   └── ground_truth/             # Corrected transcriptions (JSONL)\n",
    "│       └── {category}/\n",
    "│           └── {source}.jsonl\n",
    "└── reports/                      # Generated evaluation reports\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
