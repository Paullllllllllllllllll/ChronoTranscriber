{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoTranscriber Evaluation: CER & WER Analysis\n",
    "\n",
    "This notebook evaluates transcription quality across multiple LLM models by computing:\n",
    "- **Character Error Rate (CER)**: Edit distance at character level\n",
    "- **Word Error Rate (WER)**: Edit distance at word level\n",
    "\n",
    "## Models Evaluated\n",
    "| Provider | Model | Reasoning |\n",
    "|----------|-------|----------|\n",
    "| OpenAI | GPT-5.1 | Medium |\n",
    "| OpenAI | GPT-5 Mini | Medium |\n",
    "| Google | Gemini 3.0 Pro | Medium |\n",
    "| Google | Gemini 2.5 Flash | None |\n",
    "| Anthropic | Claude Sonnet 4.5 | Medium |\n",
    "| Anthropic | Claude Haiku 4.5 | Medium |\n",
    "\n",
    "## Dataset Categories\n",
    "1. **Address Books** - Swiss address book pages (Basel 1900)\n",
    "2. **Bibliography** - European culinary bibliographies  \n",
    "3. **Military Records** - Brazilian military enlistment cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "EVAL_DIR = Path.cwd()\n",
    "PROJECT_ROOT = EVAL_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "\n",
    "# Import evaluation metrics\n",
    "from metrics import (\n",
    "    compute_metrics,\n",
    "    aggregate_metrics,\n",
    "    TranscriptionMetrics,\n",
    "    format_metrics_table,\n",
    ")\n",
    "\n",
    "# Data handling\n",
    "import yaml\n",
    "\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation configuration\n",
    "CONFIG_PATH = EVAL_DIR / \"eval_config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "INPUT_PATH = EVAL_DIR / config['dataset']['input_path']\n",
    "OUTPUT_PATH = EVAL_DIR / config['dataset']['output_path']\n",
    "GROUND_TRUTH_PATH = EVAL_DIR / config['dataset']['ground_truth_path']\n",
    "REPORTS_PATH = EVAL_DIR / config['evaluation']['reports_path']\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract categories and models\n",
    "CATEGORIES = [cat['name'] for cat in config['dataset']['categories']]\n",
    "MODELS = {m['name']: m for m in config['models']}\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"\\nCategories: {CATEGORIES}\")\n",
    "print(f\"\\nModels: {list(MODELS.keys())}\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Input: {INPUT_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Ground Truth: {GROUND_TRUTH_PATH}\")\n",
    "print(f\"  Reports: {REPORTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcription_from_txt(file_path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load transcription text from a .txt file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .txt file\n",
    "        \n",
    "    Returns:\n",
    "        Text content or None if file doesn't exist\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_transcription_from_jsonl(file_path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load and concatenate transcriptions from a JSONL file.\n",
    "    \n",
    "    The JSONL file may contain multiple page transcriptions.\n",
    "    We extract the 'text_chunk' or 'transcription' field from each line.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .jsonl file\n",
    "        \n",
    "    Returns:\n",
    "        Concatenated text content or None if file doesn't exist\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "    \n",
    "    transcriptions = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                # Try different field names\n",
    "                text = record.get('text_chunk') or record.get('transcription')\n",
    "                if text:\n",
    "                    transcriptions.append(text)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    if not transcriptions:\n",
    "        return None\n",
    "    \n",
    "    return \"\\n\\n\".join(transcriptions)\n",
    "\n",
    "\n",
    "def find_output_file(category: str, model_name: str, source_name: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Find the output transcription file for a given source.\n",
    "    \n",
    "    Searches for .txt or .jsonl files in the model's output directory.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category (e.g., 'address_books')\n",
    "        model_name: Model identifier (e.g., 'gpt_5_mini_medium')\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        Path to output file or None if not found\n",
    "    \"\"\"\n",
    "    model_output_dir = OUTPUT_PATH / category / model_name\n",
    "    \n",
    "    if not model_output_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Try different file patterns\n",
    "    base_name = Path(source_name).stem\n",
    "    \n",
    "    # Check for direct .txt file\n",
    "    txt_file = model_output_dir / f\"{base_name}.txt\"\n",
    "    if txt_file.exists():\n",
    "        return txt_file\n",
    "    \n",
    "    # Check for folder with transcription.txt\n",
    "    folder_txt = model_output_dir / base_name / \"transcription.txt\"\n",
    "    if folder_txt.exists():\n",
    "        return folder_txt\n",
    "    \n",
    "    # Check for JSONL file\n",
    "    jsonl_file = model_output_dir / f\"{base_name}.jsonl\"\n",
    "    if jsonl_file.exists():\n",
    "        return jsonl_file\n",
    "    \n",
    "    # Check in subfolder\n",
    "    folder_jsonl = model_output_dir / base_name / f\"{base_name}_transcription.jsonl\"\n",
    "    if folder_jsonl.exists():\n",
    "        return folder_jsonl\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_output_transcription(category: str, model_name: str, source_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load a model's output transcription for a given source.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        Transcription text or None if not found\n",
    "    \"\"\"\n",
    "    file_path = find_output_file(category, model_name, source_name)\n",
    "    \n",
    "    if file_path is None:\n",
    "        return None\n",
    "    \n",
    "    if file_path.suffix == '.jsonl':\n",
    "        return load_transcription_from_jsonl(file_path)\n",
    "    else:\n",
    "        return load_transcription_from_txt(file_path)\n",
    "\n",
    "\n",
    "def find_ground_truth_file(category: str, source_name: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Find the ground truth file for a given source.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        Path to ground truth file or None if not found\n",
    "    \"\"\"\n",
    "    gt_dir = GROUND_TRUTH_PATH / category\n",
    "    \n",
    "    if not gt_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    base_name = Path(source_name).stem\n",
    "    \n",
    "    # Check for .txt file\n",
    "    txt_file = gt_dir / f\"{base_name}.txt\"\n",
    "    if txt_file.exists():\n",
    "        return txt_file\n",
    "    \n",
    "    # Check for folder with ground_truth.txt\n",
    "    folder_txt = gt_dir / base_name / \"ground_truth.txt\"\n",
    "    if folder_txt.exists():\n",
    "        return folder_txt\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_ground_truth(category: str, source_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load ground truth transcription for a given source.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        Ground truth text or None if not found\n",
    "    \"\"\"\n",
    "    file_path = find_ground_truth_file(category, source_name)\n",
    "    \n",
    "    if file_path is None:\n",
    "        return None\n",
    "    \n",
    "    return load_transcription_from_txt(file_path)\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sources(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover source files/folders in the input directory for a category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of source names\n",
    "    \"\"\"\n",
    "    input_dir = INPUT_PATH / category\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    for item in input_dir.iterdir():\n",
    "        if item.is_file() and item.suffix.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']:\n",
    "            sources.append(item.name)\n",
    "        elif item.is_dir():\n",
    "            # Check if folder contains images\n",
    "            images = list(item.glob('*.jpg')) + list(item.glob('*.png'))\n",
    "            if images:\n",
    "                sources.append(item.name)\n",
    "    \n",
    "    return sorted(sources)\n",
    "\n",
    "\n",
    "def discover_available_models(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover which models have output for a given category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of model names with available output\n",
    "    \"\"\"\n",
    "    output_dir = OUTPUT_PATH / category\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    return sorted([d.name for d in output_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "\n",
    "# Discover and display available data\n",
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_summary = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    sources = discover_sources(category)\n",
    "    available_models = discover_available_models(category)\n",
    "    gt_available = (GROUND_TRUTH_PATH / category).exists() and any((GROUND_TRUTH_PATH / category).iterdir())\n",
    "    \n",
    "    data_summary[category] = {\n",
    "        'sources': sources,\n",
    "        'models': available_models,\n",
    "        'ground_truth_available': gt_available,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Input sources: {len(sources)}\")\n",
    "    if sources:\n",
    "        for s in sources[:5]:\n",
    "            print(f\"    - {s}\")\n",
    "        if len(sources) > 5:\n",
    "            print(f\"    ... and {len(sources) - 5} more\")\n",
    "    print(f\"  Models with output: {len(available_models)}\")\n",
    "    for m in available_models:\n",
    "        print(f\"    - {m}\")\n",
    "    print(f\"  Ground truth available: {'Yes' if gt_available else 'No (pending)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Container for evaluation results.\"\"\"\n",
    "    category: str\n",
    "    model_name: str\n",
    "    source_name: str\n",
    "    metrics: Optional[TranscriptionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def evaluate_source(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    "    source_name: str,\n",
    ") -> EvaluationResult:\n",
    "    \"\"\"\n",
    "    Evaluate a single source against ground truth.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        EvaluationResult with metrics or error info\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth(category, source_name)\n",
    "    if ground_truth is None:\n",
    "        return EvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            metrics=None,\n",
    "            ground_truth_found=False,\n",
    "            output_found=False,\n",
    "            error=\"Ground truth not found\",\n",
    "        )\n",
    "    \n",
    "    # Load model output\n",
    "    output = load_output_transcription(category, model_name, source_name)\n",
    "    if output is None:\n",
    "        return EvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=False,\n",
    "            error=\"Model output not found\",\n",
    "        )\n",
    "    \n",
    "    # Compute metrics\n",
    "    try:\n",
    "        metrics = compute_metrics(ground_truth, output, normalize=True)\n",
    "        return EvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            metrics=metrics,\n",
    "            ground_truth_found=True,\n",
    "            output_found=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return EvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=True,\n",
    "            error=str(e),\n",
    "        )\n",
    "\n",
    "\n",
    "def evaluate_model_category(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    ") -> Tuple[List[EvaluationResult], Optional[TranscriptionMetrics]]:\n",
    "    \"\"\"\n",
    "    Evaluate all sources in a category for a given model.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of per-source results, aggregated metrics)\n",
    "    \"\"\"\n",
    "    sources = discover_sources(category)\n",
    "    results = []\n",
    "    valid_metrics = []\n",
    "    \n",
    "    for source in sources:\n",
    "        result = evaluate_source(category, model_name, source)\n",
    "        results.append(result)\n",
    "        if result.metrics is not None:\n",
    "            valid_metrics.append(result.metrics)\n",
    "    \n",
    "    aggregated = aggregate_metrics(valid_metrics) if valid_metrics else None\n",
    "    \n",
    "    return results, aggregated\n",
    "\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results: Dict[str, Dict[str, List[EvaluationResult]]] = {}\n",
    "aggregated_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    all_results[category] = {}\n",
    "    aggregated_metrics[category] = {}\n",
    "    \n",
    "    available_models = discover_available_models(category)\n",
    "    \n",
    "    if not available_models:\n",
    "        print(f\"\\n{category}: No model outputs found (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in available_models:\n",
    "        results, agg_metrics = evaluate_model_category(category, model_name)\n",
    "        all_results[category][model_name] = results\n",
    "        \n",
    "        if agg_metrics:\n",
    "            aggregated_metrics[category][model_name] = agg_metrics\n",
    "            print(f\"  {model_name}:\")\n",
    "            print(f\"    CER: {agg_metrics.cer*100:.2f}%  |  WER: {agg_metrics.wer*100:.2f}%\")\n",
    "            print(f\"    Sources evaluated: {sum(1 for r in results if r.metrics)}\")\n",
    "        else:\n",
    "            errors = [r.error for r in results if r.error]\n",
    "            print(f\"  {model_name}: No valid evaluations\")\n",
    "            if errors:\n",
    "                print(f\"    Errors: {errors[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure for display: model -> category -> metrics\n",
    "model_category_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category, models in aggregated_metrics.items():\n",
    "    for model_name, metrics in models.items():\n",
    "        if model_name not in model_category_metrics:\n",
    "            model_category_metrics[model_name] = {}\n",
    "        model_category_metrics[model_name][category] = metrics\n",
    "\n",
    "# Display as formatted table\n",
    "if model_category_metrics:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RESULTS SUMMARY TABLE\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    print(format_metrics_table(model_category_metrics, CATEGORIES))\n",
    "else:\n",
    "    print(\"\\nNo evaluation results available.\")\n",
    "    print(\"\\nTo generate results:\")\n",
    "    print(\"1. Run transcriptions for each model (save outputs to test_data/output/{category}/{model_name}/)\")\n",
    "    print(\"2. Create ground truth files in test_data/ground_truth/{category}/\")\n",
    "    print(\"3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall metrics per model (across all categories)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL MODEL PERFORMANCE (All Categories Combined)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "overall_model_metrics = {}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    all_metrics = list(cat_metrics.values())\n",
    "    if all_metrics:\n",
    "        overall = aggregate_metrics(all_metrics)\n",
    "        overall_model_metrics[model_name] = overall\n",
    "\n",
    "# Sort by CER for ranking\n",
    "if overall_model_metrics:\n",
    "    ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Model':<30} {'CER (%)':<12} {'WER (%)':<12} {'Chars':<12} {'Words':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "        print(f\"{rank:<6} {model_name:<30} {metrics.cer*100:<12.2f} {metrics.wer*100:<12.2f} \"\n",
    "              f\"{metrics.ref_char_count:<12,} {metrics.ref_word_count:<10,}\")\n",
    "else:\n",
    "    print(\"No overall metrics available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Per-Source Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed results for a specific category/model (configurable)\n",
    "SHOW_CATEGORY = \"address_books\"  # Change as needed\n",
    "SHOW_MODEL = None  # Set to specific model name or None for all\n",
    "\n",
    "if SHOW_CATEGORY in all_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED RESULTS: {SHOW_CATEGORY.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    models_to_show = [SHOW_MODEL] if SHOW_MODEL else list(all_results[SHOW_CATEGORY].keys())\n",
    "    \n",
    "    for model_name in models_to_show:\n",
    "        if model_name not in all_results[SHOW_CATEGORY]:\n",
    "            continue\n",
    "            \n",
    "        results = all_results[SHOW_CATEGORY][model_name]\n",
    "        \n",
    "        print(f\"\\n{model_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Source':<40} {'CER (%)':<12} {'WER (%)':<12} {'Status'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for result in results:\n",
    "            source_display = result.source_name[:38] + \"..\" if len(result.source_name) > 40 else result.source_name\n",
    "            \n",
    "            if result.metrics:\n",
    "                print(f\"{source_display:<40} {result.metrics.cer*100:<12.2f} {result.metrics.wer*100:<12.2f} OK\")\n",
    "            else:\n",
    "                status = result.error or \"Unknown error\"\n",
    "                print(f\"{source_display:<40} {'--':<12} {'--':<12} {status}\")\n",
    "else:\n",
    "    print(f\"Category '{SHOW_CATEGORY}' not found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp for reports\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export aggregated metrics to JSON\n",
    "json_report = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"categories\": CATEGORIES,\n",
    "    \"models\": list(MODELS.keys()),\n",
    "    \"results\": {},\n",
    "}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    json_report[\"results\"][model_name] = {\n",
    "        \"per_category\": {cat: m.to_dict() for cat, m in cat_metrics.items()},\n",
    "    }\n",
    "    if model_name in overall_model_metrics:\n",
    "        json_report[\"results\"][model_name][\"overall\"] = overall_model_metrics[model_name].to_dict()\n",
    "\n",
    "json_path = REPORTS_PATH / f\"eval_results_{timestamp}.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_report, f, indent=2)\n",
    "print(f\"JSON report saved: {json_path}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = REPORTS_PATH / f\"eval_results_{timestamp}.csv\"\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Category\", \"CER (%)\", \"WER (%)\", \"Char Distance\", \"Word Distance\", \n",
    "                     \"Ref Chars\", \"Ref Words\", \"Hyp Chars\", \"Hyp Words\"])\n",
    "    \n",
    "    for model_name, cat_metrics in model_category_metrics.items():\n",
    "        for category, metrics in cat_metrics.items():\n",
    "            writer.writerow([\n",
    "                model_name,\n",
    "                category,\n",
    "                round(metrics.cer * 100, 2),\n",
    "                round(metrics.wer * 100, 2),\n",
    "                metrics.char_distance,\n",
    "                metrics.word_distance,\n",
    "                metrics.ref_char_count,\n",
    "                metrics.ref_word_count,\n",
    "                metrics.hyp_char_count,\n",
    "                metrics.hyp_word_count,\n",
    "            ])\n",
    "\n",
    "print(f\"CSV report saved: {csv_path}\")\n",
    "\n",
    "# Export Markdown summary\n",
    "md_path = REPORTS_PATH / f\"eval_results_{timestamp}.md\"\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# ChronoTranscriber Evaluation Results\\n\\n\")\n",
    "    f.write(f\"**Generated:** {timestamp}\\n\\n\")\n",
    "    f.write(f\"## Models Evaluated\\n\\n\")\n",
    "    for name, info in MODELS.items():\n",
    "        f.write(f\"- **{name}**: {info.get('description', '')}\\n\")\n",
    "    f.write(f\"\\n## Results by Category\\n\\n\")\n",
    "    f.write(format_metrics_table(model_category_metrics, CATEGORIES))\n",
    "    f.write(f\"\\n\\n## Overall Rankings\\n\\n\")\n",
    "    if overall_model_metrics:\n",
    "        ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "        f.write(\"| Rank | Model | CER (%) | WER (%) |\\n\")\n",
    "        f.write(\"|------|-------|---------|---------|\\n\")\n",
    "        for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "            f.write(f\"| {rank} | {model_name} | {metrics.cer*100:.2f} | {metrics.wer*100:.2f} |\\n\")\n",
    "\n",
    "print(f\"Markdown report saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create visualizations if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    PLOT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOT_AVAILABLE = False\n",
    "    print(\"matplotlib not available - skipping visualizations\")\n",
    "    print(\"Install with: pip install matplotlib\")\n",
    "\n",
    "if PLOT_AVAILABLE and overall_model_metrics:\n",
    "    # Prepare data\n",
    "    models = list(overall_model_metrics.keys())\n",
    "    cer_values = [overall_model_metrics[m].cer * 100 for m in models]\n",
    "    wer_values = [overall_model_metrics[m].wer * 100 for m in models]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # CER bar chart\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.barh(models, cer_values, color='steelblue')\n",
    "    ax1.set_xlabel('Character Error Rate (%)')\n",
    "    ax1.set_title('CER by Model')\n",
    "    ax1.bar_label(bars1, fmt='%.2f%%', padding=3)\n",
    "    ax1.set_xlim(0, max(cer_values) * 1.2 if cer_values else 10)\n",
    "    \n",
    "    # WER bar chart\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(models, wer_values, color='darkorange')\n",
    "    ax2.set_xlabel('Word Error Rate (%)')\n",
    "    ax2.set_title('WER by Model')\n",
    "    ax2.bar_label(bars2, fmt='%.2f%%', padding=3)\n",
    "    ax2.set_xlim(0, max(wer_values) * 1.2 if wer_values else 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = REPORTS_PATH / f\"eval_chart_{timestamp}.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Chart saved: {fig_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### To complete the evaluation:\n",
    "\n",
    "1. **Create Ground Truth**\n",
    "   - Run a transcription pass on each category using a high-quality model\n",
    "   - Manually review and correct the outputs\n",
    "   - Save corrected files to `test_data/ground_truth/{category}/{source_name}.txt`\n",
    "\n",
    "2. **Run Model Transcriptions**\n",
    "   - For each model, transcribe all sources in each category\n",
    "   - Save outputs to `test_data/output/{category}/{model_name}/`\n",
    "   - Use the ChronoTranscriber CLI with appropriate model configuration\n",
    "\n",
    "3. **Re-run This Notebook**\n",
    "   - The notebook will automatically discover outputs and compute metrics\n",
    "   - Results will be exported to the `reports/` directory\n",
    "\n",
    "### Expected output structure:\n",
    "```\n",
    "eval/\n",
    "├── test_data/\n",
    "│   ├── input/\n",
    "│   │   ├── address_books/      # Source images\n",
    "│   │   ├── bibliography/       # Source PDFs\n",
    "│   │   └── military_records/   # Source PDFs\n",
    "│   ├── output/\n",
    "│   │   ├── address_books/\n",
    "│   │   │   ├── gpt_5.1_medium/\n",
    "│   │   │   ├── gpt_5_mini_medium/\n",
    "│   │   │   ├── gemini_3.0_pro_medium/\n",
    "│   │   │   └── ...\n",
    "│   │   └── ...\n",
    "│   └── ground_truth/\n",
    "│       ├── address_books/      # Manually corrected transcriptions\n",
    "│       ├── bibliography/\n",
    "│       └── military_records/\n",
    "└── reports/\n",
    "    ├── eval_results_YYYYMMDD_HHMMSS.json\n",
    "    ├── eval_results_YYYYMMDD_HHMMSS.csv\n",
    "    ├── eval_results_YYYYMMDD_HHMMSS.md\n",
    "    └── eval_chart_YYYYMMDD_HHMMSS.png\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
