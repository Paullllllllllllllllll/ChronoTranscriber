{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoTranscriber Evaluation: CER & WER Analysis\n",
    "\n",
    "This notebook evaluates transcription quality across multiple LLM models by computing:\n",
    "- **Character Error Rate (CER)**: Edit distance at character level\n",
    "- **Word Error Rate (WER)**: Edit distance at word level\n",
    "\n",
    "## Evaluation Method\n",
    "Metrics are computed **page-by-page** using the temporary JSONL files produced by the transcriber.\n",
    "This ensures accurate comparison without formatting penalties from post-processing.\n",
    "\n",
    "## Models Evaluated\n",
    "| Provider | Model | Reasoning |\n",
    "|----------|-------|----------|\n",
    "| OpenAI | GPT-5.1 | Medium |\n",
    "| OpenAI | GPT-5 Mini | Medium |\n",
    "| Google | Gemini 3.0 Pro | Medium |\n",
    "| Google | Gemini 2.5 Flash | None |\n",
    "| Anthropic | Claude Sonnet 4.5 | Medium |\n",
    "| Anthropic | Claude Haiku 4.5 | Medium |\n",
    "\n",
    "## Dataset Categories\n",
    "1. **Address Books** - Swiss address book pages (Basel 1900)\n",
    "2. **Bibliography** - European culinary bibliographies  \n",
    "3. **Military Records** - Brazilian military enlistment cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "EVAL_DIR = Path.cwd()\n",
    "PROJECT_ROOT = EVAL_DIR.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "sys.path.insert(0, str(EVAL_DIR))\n",
    "\n",
    "# Import evaluation metrics\n",
    "from metrics import (\n",
    "    compute_metrics,\n",
    "    aggregate_metrics,\n",
    "    TranscriptionMetrics,\n",
    "    format_metrics_table,\n",
    ")\n",
    "\n",
    "# Import JSONL page-level utilities\n",
    "from jsonl_eval import (\n",
    "    PageTranscription,\n",
    "    DocumentTranscriptions,\n",
    "    parse_transcription_jsonl,\n",
    "    find_jsonl_file,\n",
    "    load_page_transcriptions,\n",
    "    load_ground_truth_pages,\n",
    "    align_pages,\n",
    ")\n",
    "\n",
    "# Data handling\n",
    "import yaml\n",
    "\n",
    "print(f\"Evaluation directory: {EVAL_DIR}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation configuration\n",
    "CONFIG_PATH = EVAL_DIR / \"eval_config.yaml\"\n",
    "\n",
    "with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract paths\n",
    "INPUT_PATH = EVAL_DIR / config['dataset']['input_path']\n",
    "OUTPUT_PATH = EVAL_DIR / config['dataset']['output_path']\n",
    "GROUND_TRUTH_PATH = EVAL_DIR / config['dataset']['ground_truth_path']\n",
    "REPORTS_PATH = EVAL_DIR / config['evaluation']['reports_path']\n",
    "\n",
    "# Create reports directory\n",
    "REPORTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Extract categories and models\n",
    "CATEGORIES = [cat['name'] for cat in config['dataset']['categories']]\n",
    "MODELS = {m['name']: m for m in config['models']}\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"\\nCategories: {CATEGORIES}\")\n",
    "print(f\"\\nModels: {list(MODELS.keys())}\")\n",
    "print(f\"\\nPaths:\")\n",
    "print(f\"  Input: {INPUT_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Ground Truth: {GROUND_TRUTH_PATH}\")\n",
    "print(f\"  Reports: {REPORTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discover Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sources(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover source files/folders in the input directory for a category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of source names\n",
    "    \"\"\"\n",
    "    input_dir = INPUT_PATH / category\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    sources = []\n",
    "    \n",
    "    for item in input_dir.iterdir():\n",
    "        if item.is_file() and item.suffix.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']:\n",
    "            sources.append(item.name)\n",
    "        elif item.is_dir():\n",
    "            # Check if folder contains images\n",
    "            images = list(item.glob('*.jpg')) + list(item.glob('*.png'))\n",
    "            if images:\n",
    "                sources.append(item.name)\n",
    "    \n",
    "    return sorted(sources)\n",
    "\n",
    "\n",
    "def discover_available_models(category: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Discover which models have JSONL output for a given category.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        \n",
    "    Returns:\n",
    "        List of model names with available output\n",
    "    \"\"\"\n",
    "    output_dir = OUTPUT_PATH / category\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    models = []\n",
    "    for d in output_dir.iterdir():\n",
    "        if d.is_dir():\n",
    "            # Check if model directory has any JSONL files\n",
    "            jsonl_files = list(d.rglob('*.jsonl'))\n",
    "            if jsonl_files:\n",
    "                models.append(d.name)\n",
    "    \n",
    "    return sorted(models)\n",
    "\n",
    "\n",
    "def check_ground_truth_available(category: str) -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Check if ground truth JSONL files exist for a category.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (has_ground_truth, count_of_files)\n",
    "    \"\"\"\n",
    "    gt_dir = GROUND_TRUTH_PATH / category\n",
    "    if not gt_dir.exists():\n",
    "        return False, 0\n",
    "    \n",
    "    jsonl_files = list(gt_dir.glob('*.jsonl'))\n",
    "    return len(jsonl_files) > 0, len(jsonl_files)\n",
    "\n",
    "\n",
    "# Discover and display available data\n",
    "print(\"=\" * 60)\n",
    "print(\"AVAILABLE DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_summary = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    sources = discover_sources(category)\n",
    "    available_models = discover_available_models(category)\n",
    "    gt_available, gt_count = check_ground_truth_available(category)\n",
    "    \n",
    "    data_summary[category] = {\n",
    "        'sources': sources,\n",
    "        'models': available_models,\n",
    "        'ground_truth_available': gt_available,\n",
    "        'ground_truth_count': gt_count,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Input sources: {len(sources)}\")\n",
    "    if sources:\n",
    "        for s in sources[:5]:\n",
    "            print(f\"    - {s}\")\n",
    "        if len(sources) > 5:\n",
    "            print(f\"    ... and {len(sources) - 5} more\")\n",
    "    print(f\"  Models with JSONL output: {len(available_models)}\")\n",
    "    for m in available_models:\n",
    "        print(f\"    - {m}\")\n",
    "    print(f\"  Ground truth JSONL: {'Yes' if gt_available else 'No'} ({gt_count} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Page-Level Evaluation\n",
    "\n",
    "Metrics are computed by comparing each page of the model output against the corresponding\n",
    "ground truth page. This ensures:\n",
    "- No formatting penalties from whitespace differences in final TXT output\n",
    "- Accurate per-page error attribution\n",
    "- Better isolation of transcription quality from post-processing effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageEvaluationResult:\n",
    "    \"\"\"Container for per-page evaluation results.\"\"\"\n",
    "    page_index: int\n",
    "    image_name: str\n",
    "    metrics: Optional[TranscriptionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SourceEvaluationResult:\n",
    "    \"\"\"Container for source-level evaluation results.\"\"\"\n",
    "    category: str\n",
    "    model_name: str\n",
    "    source_name: str\n",
    "    page_results: List[PageEvaluationResult]\n",
    "    aggregated_metrics: Optional[TranscriptionMetrics]\n",
    "    ground_truth_found: bool\n",
    "    output_found: bool\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def total_pages(self) -> int:\n",
    "        return len(self.page_results)\n",
    "    \n",
    "    @property\n",
    "    def evaluated_pages(self) -> int:\n",
    "        return sum(1 for p in self.page_results if p.metrics is not None)\n",
    "\n",
    "\n",
    "def evaluate_source_pages(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    "    source_name: str,\n",
    ") -> SourceEvaluationResult:\n",
    "    \"\"\"\n",
    "    Evaluate a source by comparing pages from model output to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        source_name: Source file/folder name\n",
    "        \n",
    "    Returns:\n",
    "        SourceEvaluationResult with per-page and aggregated metrics\n",
    "    \"\"\"\n",
    "    # Load ground truth pages\n",
    "    gt_doc = load_ground_truth_pages(GROUND_TRUTH_PATH, category, source_name)\n",
    "    if gt_doc is None or not gt_doc.pages:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            page_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=False,\n",
    "            output_found=False,\n",
    "            error=\"Ground truth JSONL not found\",\n",
    "        )\n",
    "    \n",
    "    # Load model output pages\n",
    "    hyp_doc = load_page_transcriptions(OUTPUT_PATH, category, model_name, source_name)\n",
    "    if hyp_doc is None or not hyp_doc.pages:\n",
    "        return SourceEvaluationResult(\n",
    "            category=category,\n",
    "            model_name=model_name,\n",
    "            source_name=source_name,\n",
    "            page_results=[],\n",
    "            aggregated_metrics=None,\n",
    "            ground_truth_found=True,\n",
    "            output_found=False,\n",
    "            error=\"Model output JSONL not found\",\n",
    "        )\n",
    "    \n",
    "    # Align pages\n",
    "    aligned = align_pages(hyp_doc, gt_doc)\n",
    "    \n",
    "    # Compute per-page metrics\n",
    "    page_results: List[PageEvaluationResult] = []\n",
    "    valid_metrics: List[TranscriptionMetrics] = []\n",
    "    \n",
    "    for hyp_page, gt_page in aligned:\n",
    "        # Determine page info\n",
    "        if gt_page:\n",
    "            page_index = gt_page.page_index\n",
    "            image_name = gt_page.image_name or (hyp_page.image_name if hyp_page else \"\")\n",
    "        elif hyp_page:\n",
    "            page_index = hyp_page.page_index\n",
    "            image_name = hyp_page.image_name\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check availability\n",
    "        gt_found = gt_page is not None and gt_page.has_text()\n",
    "        hyp_found = hyp_page is not None and hyp_page.has_text()\n",
    "        \n",
    "        if not gt_found:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=False,\n",
    "                output_found=hyp_found,\n",
    "                error=\"No ground truth for page\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        if not hyp_found:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=False,\n",
    "                error=\"No model output for page\",\n",
    "            ))\n",
    "            continue\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            metrics = compute_metrics(\n",
    "                gt_page.transcription,\n",
    "                hyp_page.transcription,\n",
    "                normalize=True,\n",
    "            )\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=metrics,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "            ))\n",
    "            valid_metrics.append(metrics)\n",
    "        except Exception as e:\n",
    "            page_results.append(PageEvaluationResult(\n",
    "                page_index=page_index,\n",
    "                image_name=image_name,\n",
    "                metrics=None,\n",
    "                ground_truth_found=True,\n",
    "                output_found=True,\n",
    "                error=str(e),\n",
    "            ))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    aggregated = aggregate_metrics(valid_metrics) if valid_metrics else None\n",
    "    \n",
    "    return SourceEvaluationResult(\n",
    "        category=category,\n",
    "        model_name=model_name,\n",
    "        source_name=source_name,\n",
    "        page_results=page_results,\n",
    "        aggregated_metrics=aggregated,\n",
    "        ground_truth_found=True,\n",
    "        output_found=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model_category(\n",
    "    category: str,\n",
    "    model_name: str,\n",
    ") -> Tuple[List[SourceEvaluationResult], Optional[TranscriptionMetrics]]:\n",
    "    \"\"\"\n",
    "    Evaluate all sources in a category for a given model.\n",
    "    \n",
    "    Args:\n",
    "        category: Dataset category\n",
    "        model_name: Model identifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of per-source results, aggregated metrics)\n",
    "    \"\"\"\n",
    "    sources = discover_sources(category)\n",
    "    results = []\n",
    "    all_page_metrics = []\n",
    "    \n",
    "    for source in sources:\n",
    "        result = evaluate_source_pages(category, model_name, source)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Collect valid page metrics for aggregation\n",
    "        for page_result in result.page_results:\n",
    "            if page_result.metrics is not None:\n",
    "                all_page_metrics.append(page_result.metrics)\n",
    "    \n",
    "    aggregated = aggregate_metrics(all_page_metrics) if all_page_metrics else None\n",
    "    \n",
    "    return results, aggregated\n",
    "\n",
    "\n",
    "print(\"Page-level evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING PAGE-LEVEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results: Dict[str, Dict[str, List[SourceEvaluationResult]]] = {}\n",
    "aggregated_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    all_results[category] = {}\n",
    "    aggregated_metrics[category] = {}\n",
    "    \n",
    "    available_models = discover_available_models(category)\n",
    "    gt_available, gt_count = check_ground_truth_available(category)\n",
    "    \n",
    "    if not available_models:\n",
    "        print(f\"\\n{category}: No model outputs found (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    if not gt_available:\n",
    "        print(f\"\\n{category}: No ground truth JSONL files (skipping)\")\n",
    "        print(f\"  Hint: Run 'python main/prepare_ground_truth.py --extract' to create editable files\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name in available_models:\n",
    "        results, agg_metrics = evaluate_model_category(category, model_name)\n",
    "        all_results[category][model_name] = results\n",
    "        \n",
    "        if agg_metrics:\n",
    "            aggregated_metrics[category][model_name] = agg_metrics\n",
    "            total_pages = sum(r.total_pages for r in results)\n",
    "            eval_pages = sum(r.evaluated_pages for r in results)\n",
    "            print(f\"  {model_name}:\")\n",
    "            print(f\"    CER: {agg_metrics.cer*100:.2f}%  |  WER: {agg_metrics.wer*100:.2f}%\")\n",
    "            print(f\"    Pages evaluated: {eval_pages}/{total_pages}\")\n",
    "        else:\n",
    "            errors = [r.error for r in results if r.error]\n",
    "            print(f\"  {model_name}: No valid evaluations\")\n",
    "            if errors:\n",
    "                print(f\"    Error: {errors[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure for display: model -> category -> metrics\n",
    "model_category_metrics: Dict[str, Dict[str, TranscriptionMetrics]] = {}\n",
    "\n",
    "for category, models in aggregated_metrics.items():\n",
    "    for model_name, metrics in models.items():\n",
    "        if model_name not in model_category_metrics:\n",
    "            model_category_metrics[model_name] = {}\n",
    "        model_category_metrics[model_name][category] = metrics\n",
    "\n",
    "# Display as formatted table\n",
    "if model_category_metrics:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RESULTS SUMMARY TABLE\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    print(format_metrics_table(model_category_metrics, CATEGORIES))\n",
    "else:\n",
    "    print(\"\\nNo evaluation results available.\")\n",
    "    print(\"\\nTo generate results:\")\n",
    "    print(\"1. Run transcriptions for each model (outputs go to test_data/output/{category}/{model_name}/)\")\n",
    "    print(\"2. Create ground truth JSONL files:\")\n",
    "    print(\"   a. python main/prepare_ground_truth.py --extract --input test_data/output/{category}/{model}\")\n",
    "    print(\"   b. Edit the generated _editable.txt files\")\n",
    "    print(\"   c. python main/prepare_ground_truth.py --apply --input {edited_file}\")\n",
    "    print(\"3. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall metrics per model (across all categories)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERALL MODEL PERFORMANCE (All Categories Combined)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "overall_model_metrics = {}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    all_metrics = list(cat_metrics.values())\n",
    "    if all_metrics:\n",
    "        overall = aggregate_metrics(all_metrics)\n",
    "        overall_model_metrics[model_name] = overall\n",
    "\n",
    "# Sort by CER for ranking\n",
    "if overall_model_metrics:\n",
    "    ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "    \n",
    "    print(f\"{'Rank':<6} {'Model':<30} {'CER (%)':<12} {'WER (%)':<12} {'Chars':<12} {'Words':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "        print(f\"{rank:<6} {model_name:<30} {metrics.cer*100:<12.2f} {metrics.wer*100:<12.2f} \"\n",
    "              f\"{metrics.ref_char_count:<12,} {metrics.ref_word_count:<10,}\")\n",
    "else:\n",
    "    print(\"No overall metrics available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Per-Page Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed page-level results for a specific category/model (configurable)\n",
    "SHOW_CATEGORY = \"address_books\"  # Change as needed\n",
    "SHOW_MODEL = None  # Set to specific model name or None for first available\n",
    "SHOW_SOURCE = None  # Set to specific source name or None for first available\n",
    "\n",
    "if SHOW_CATEGORY in all_results and all_results[SHOW_CATEGORY]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED PAGE-LEVEL RESULTS: {SHOW_CATEGORY.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    models_to_show = [SHOW_MODEL] if SHOW_MODEL else list(all_results[SHOW_CATEGORY].keys())[:1]\n",
    "    \n",
    "    for model_name in models_to_show:\n",
    "        if model_name not in all_results[SHOW_CATEGORY]:\n",
    "            continue\n",
    "            \n",
    "        results = all_results[SHOW_CATEGORY][model_name]\n",
    "        sources_to_show = [r for r in results if r.source_name == SHOW_SOURCE] if SHOW_SOURCE else results[:1]\n",
    "        \n",
    "        for source_result in sources_to_show:\n",
    "            print(f\"\\n{model_name} / {source_result.source_name}\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"{'Page':<6} {'Image':<35} {'CER (%)':<12} {'WER (%)':<12} {'Status'}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for page_result in source_result.page_results:\n",
    "                page_num = page_result.page_index + 1\n",
    "                img_display = page_result.image_name[:33] + \"..\" if len(page_result.image_name) > 35 else page_result.image_name\n",
    "                \n",
    "                if page_result.metrics:\n",
    "                    print(f\"{page_num:<6} {img_display:<35} {page_result.metrics.cer*100:<12.2f} {page_result.metrics.wer*100:<12.2f} OK\")\n",
    "                else:\n",
    "                    status = page_result.error or \"Error\"\n",
    "                    print(f\"{page_num:<6} {img_display:<35} {'--':<12} {'--':<12} {status}\")\n",
    "            \n",
    "            if source_result.aggregated_metrics:\n",
    "                m = source_result.aggregated_metrics\n",
    "                print(\"-\" * 70)\n",
    "                print(f\"{'TOTAL':<6} {'':<35} {m.cer*100:<12.2f} {m.wer*100:<12.2f}\")\n",
    "else:\n",
    "    print(f\"Category '{SHOW_CATEGORY}' not found in results or has no evaluated models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp for reports\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export aggregated metrics to JSON\n",
    "json_report = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"evaluation_method\": \"page_level_jsonl\",\n",
    "    \"categories\": CATEGORIES,\n",
    "    \"models\": list(MODELS.keys()),\n",
    "    \"results\": {},\n",
    "}\n",
    "\n",
    "for model_name, cat_metrics in model_category_metrics.items():\n",
    "    json_report[\"results\"][model_name] = {\n",
    "        \"per_category\": {cat: m.to_dict() for cat, m in cat_metrics.items()},\n",
    "    }\n",
    "    if model_name in overall_model_metrics:\n",
    "        json_report[\"results\"][model_name][\"overall\"] = overall_model_metrics[model_name].to_dict()\n",
    "\n",
    "json_path = REPORTS_PATH / f\"eval_results_{timestamp}.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_report, f, indent=2)\n",
    "print(f\"JSON report saved: {json_path}\")\n",
    "\n",
    "# Export to CSV (per-page detail)\n",
    "csv_path = REPORTS_PATH / f\"eval_results_{timestamp}.csv\"\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model\", \"Category\", \"Source\", \"Page\", \"Image\", \"CER (%)\", \"WER (%)\", \n",
    "                     \"Char Distance\", \"Ref Chars\", \"Status\"])\n",
    "    \n",
    "    for category, models in all_results.items():\n",
    "        for model_name, sources in models.items():\n",
    "            for source_result in sources:\n",
    "                for page_result in source_result.page_results:\n",
    "                    if page_result.metrics:\n",
    "                        writer.writerow([\n",
    "                            model_name,\n",
    "                            category,\n",
    "                            source_result.source_name,\n",
    "                            page_result.page_index + 1,\n",
    "                            page_result.image_name,\n",
    "                            round(page_result.metrics.cer * 100, 2),\n",
    "                            round(page_result.metrics.wer * 100, 2),\n",
    "                            page_result.metrics.char_distance,\n",
    "                            page_result.metrics.ref_char_count,\n",
    "                            \"OK\",\n",
    "                        ])\n",
    "                    else:\n",
    "                        writer.writerow([\n",
    "                            model_name,\n",
    "                            category,\n",
    "                            source_result.source_name,\n",
    "                            page_result.page_index + 1,\n",
    "                            page_result.image_name,\n",
    "                            \"\",\n",
    "                            \"\",\n",
    "                            \"\",\n",
    "                            \"\",\n",
    "                            page_result.error or \"Error\",\n",
    "                        ])\n",
    "\n",
    "print(f\"CSV report saved: {csv_path}\")\n",
    "\n",
    "# Export Markdown summary\n",
    "md_path = REPORTS_PATH / f\"eval_results_{timestamp}.md\"\n",
    "with open(md_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"# ChronoTranscriber Evaluation Results\\n\\n\")\n",
    "    f.write(f\"**Generated:** {timestamp}\\n\\n\")\n",
    "    f.write(f\"**Evaluation Method:** Page-level JSONL comparison\\n\\n\")\n",
    "    f.write(f\"## Models Evaluated\\n\\n\")\n",
    "    for name, info in MODELS.items():\n",
    "        f.write(f\"- **{name}**: {info.get('description', '')}\\n\")\n",
    "    f.write(f\"\\n## Results by Category\\n\\n\")\n",
    "    f.write(format_metrics_table(model_category_metrics, CATEGORIES))\n",
    "    f.write(f\"\\n\\n## Overall Rankings\\n\\n\")\n",
    "    if overall_model_metrics:\n",
    "        ranked = sorted(overall_model_metrics.items(), key=lambda x: x[1].cer)\n",
    "        f.write(\"| Rank | Model | CER (%) | WER (%) |\\n\")\n",
    "        f.write(\"|------|-------|---------|---------|\\n\")\n",
    "        for rank, (model_name, metrics) in enumerate(ranked, 1):\n",
    "            f.write(f\"| {rank} | {model_name} | {metrics.cer*100:.2f} | {metrics.wer*100:.2f} |\\n\")\n",
    "\n",
    "print(f\"Markdown report saved: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create visualizations if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    PLOT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PLOT_AVAILABLE = False\n",
    "    print(\"matplotlib not available - skipping visualizations\")\n",
    "    print(\"Install with: pip install matplotlib\")\n",
    "\n",
    "if PLOT_AVAILABLE and overall_model_metrics:\n",
    "    # Prepare data\n",
    "    models = list(overall_model_metrics.keys())\n",
    "    cer_values = [overall_model_metrics[m].cer * 100 for m in models]\n",
    "    wer_values = [overall_model_metrics[m].wer * 100 for m in models]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # CER bar chart\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.barh(models, cer_values, color='steelblue')\n",
    "    ax1.set_xlabel('Character Error Rate (%)')\n",
    "    ax1.set_title('CER by Model')\n",
    "    ax1.bar_label(bars1, fmt='%.2f%%', padding=3)\n",
    "    ax1.set_xlim(0, max(cer_values) * 1.2 if cer_values else 10)\n",
    "    \n",
    "    # WER bar chart\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(models, wer_values, color='darkorange')\n",
    "    ax2.set_xlabel('Word Error Rate (%)')\n",
    "    ax2.set_title('WER by Model')\n",
    "    ax2.bar_label(bars2, fmt='%.2f%%', padding=3)\n",
    "    ax2.set_xlim(0, max(wer_values) * 1.2 if wer_values else 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = REPORTS_PATH / f\"eval_chart_{timestamp}.png\"\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Chart saved: {fig_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Ground Truth Workflow\n",
    "\n",
    "1. **Extract transcriptions for editing**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --extract --input eval/test_data/output/{category}/{model}\n",
    "   ```\n",
    "\n",
    "2. **Edit the generated `_editable.txt` files**\n",
    "   - Each page is marked with `=== page NNN ===`\n",
    "   - Correct transcription errors directly in the text\n",
    "   - Use `[NO TRANSCRIBABLE TEXT]` for blank pages\n",
    "   - Use `[TRANSCRIPTION NOT POSSIBLE]` for illegible pages\n",
    "\n",
    "3. **Apply corrections to create ground truth**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --apply --input eval/test_data/output/{category}/{model}\n",
    "   ```\n",
    "\n",
    "4. **Check ground truth status**\n",
    "   ```bash\n",
    "   python main/prepare_ground_truth.py --status\n",
    "   ```\n",
    "\n",
    "### Expected Directory Structure\n",
    "```\n",
    "eval/\n",
    "\u251c\u2500\u2500 test_data/\n",
    "\u2502   \u251c\u2500\u2500 input/                    # Source documents\n",
    "\u2502   \u2502   \u251c\u2500\u2500 address_books/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 bibliography/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 military_records/\n",
    "\u2502   \u251c\u2500\u2500 output/                   # Model outputs (JSONL per source)\n",
    "\u2502   \u2502   \u2514\u2500\u2500 {category}/\n",
    "\u2502   \u2502       \u2514\u2500\u2500 {model_name}/\n",
    "\u2502   \u2502           \u2514\u2500\u2500 {source}/\n",
    "\u2502   \u2502               \u2514\u2500\u2500 {source}.jsonl\n",
    "\u2502   \u2514\u2500\u2500 ground_truth/             # Corrected transcriptions (JSONL)\n",
    "\u2502       \u2514\u2500\u2500 {category}/\n",
    "\u2502           \u2514\u2500\u2500 {source}.jsonl\n",
    "\u2514\u2500\u2500 reports/                      # Generated evaluation reports\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}