# modules/image_utils.py

from __future__ import annotations

from PIL import Image, ImageOps
from pathlib import Path
from typing import List, Optional, Tuple
import numpy as np
import cv2
from deskew import determine_skew
from skimage.filters import threshold_sauvola
from modules.core.safe_paths import create_safe_directory_name, create_safe_filename

from modules.config.service import get_config_service
from modules.config.constants import SUPPORTED_IMAGE_EXTENSIONS
from modules.processing.model_utils import detect_model_type, get_image_config_section_name
from modules.infra.logger import setup_logger
from modules.infra.multiprocessing_utils import run_multiprocessing_tasks

logger = setup_logger(__name__)


class ImageProcessor:
    def __init__(self, image_path: Path, provider: str = "openai", model_name: str = "") -> None:
        """Initialize ImageProcessor with provider-specific config.
        
        Args:
            image_path: Path to the image file
            provider: Provider name (openai, google, anthropic, openrouter) or 'tesseract'
            model_name: Model name for detecting underlying model type when using OpenRouter
                       (e.g., 'google/gemini-2.5-flash' uses Google config even via OpenRouter)
        """
        if image_path.suffix.lower() not in SUPPORTED_IMAGE_EXTENSIONS:
            logger.error(f"Unsupported image format: {image_path.suffix}")
            raise ValueError(f"Unsupported image format: {image_path.suffix}")
        self.image_path = image_path
        self.provider = provider.lower()
        self.model_name = model_name.lower() if model_name else ""
        
        # Detect underlying model type from model name (for OpenRouter passthrough)
        self.model_type = detect_model_type(self.provider, self.model_name)

        # Full config dict (contains 'image_processing' and 'ocr' sections)
        self.image_config = get_config_service().get_image_processing_config()
        
        # Get provider-specific config section
        section_name = get_image_config_section_name(self.model_type)
        self.img_cfg = self.image_config.get(section_name, {})
    
    def convert_to_grayscale(self, image: Image.Image) -> Image.Image:
        """
        Convert the image to grayscale if enabled.
        Returns:
            Image.Image: The grayscale image.
        """
        if self.img_cfg.get('grayscale_conversion', True):
            return ImageOps.grayscale(image)
        return image

    @staticmethod
    def resize_for_detail(image: Image.Image, detail: str, img_cfg: dict, model_type: str = "openai") -> Image.Image:
        """
        Resize strategy based on desired LLM detail and model type.
        - low: downscale longest side to low_max_side_px.
        - high: fit/pad into high_target_box [width, height] (OpenAI/Google) or
                cap longest side to high_max_side_px (Anthropic).
        - auto: default to 'high' strategy.
        
        Args:
            image: PIL Image to resize
            detail: Detail level ('low', 'high', 'auto')
            img_cfg: Config dict for the provider
            model_type: 'openai', 'google', or 'anthropic' for provider-specific resizing
        """
        # Normalize flags and defaults
        resize_profile = (img_cfg.get('resize_profile', 'auto') or 'auto').lower()
        if resize_profile == 'none':
            return image
        detail_norm = (detail or 'high').lower()
        if detail_norm not in ('low', 'high', 'auto', 'medium', 'ultra_high'):
            detail_norm = 'high'
        
        # Low detail: cap longest side (same for all providers)
        if detail_norm == 'low':
            max_side = int(img_cfg.get('low_max_side_px', 512))
            w, h = image.size
            longest = max(w, h)
            if longest <= max_side:
                return image
            scale = max_side / float(longest)
            new_size = (max(1, int(w * scale)), max(1, int(h * scale)))
            return image.resize(new_size, Image.Resampling.LANCZOS)
        
        # High/auto/medium/ultra_high: provider-specific strategy
        if model_type == "anthropic":
            # Anthropic: cap longest side to high_max_side_px (no padding)
            max_side = int(img_cfg.get('high_max_side_px', 1568))
            w, h = image.size
            longest = max(w, h)
            if longest <= max_side:
                return image
            scale = max_side / float(longest)
            new_size = (max(1, int(w * scale)), max(1, int(h * scale)))
            return image.resize(new_size, Image.Resampling.LANCZOS)
        else:
            # OpenAI/Google: fit into box and pad with white
            box = img_cfg.get('high_target_box', [768, 1536])
            try:
                target_width = int(box[0])
                target_height = int(box[1])
            except Exception:
                target_width, target_height = 768, 1536
            orig_width, orig_height = image.size
            scale_w = target_width / orig_width
            scale_h = target_height / orig_height
            scale = min(scale_w, scale_h)
            new_width = max(1, int(orig_width * scale))
            new_height = max(1, int(orig_height * scale))
            resized_img = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            final_img = Image.new("RGB", (target_width, target_height), (255, 255, 255))
            paste_x = (target_width - new_width) // 2
            paste_y = (target_height - new_height) // 2
            final_img.paste(resized_img, (paste_x, paste_y))
            return final_img

    def handle_transparency(self, image: Image.Image) -> Image.Image:
        """
        Handle transparency by pasting the image onto a white background.
        Returns:
            Image.Image: The processed image.
        """
        if self.img_cfg.get('handle_transparency', True):
            if image.mode in ('RGBA', 'LA') or (
                    image.mode == 'P' and 'transparency' in image.info):
                background = Image.new("RGB", image.size, (255, 255, 255))
                background.paste(image, mask=image.split()[-1])
                return background
        return image

    def process_image(self, output_path: Path) -> str:
        """
        Process the image and save it to the given output path with compression.
        Returns:
            str: A message indicating the outcome.
        """
        try:
            with Image.open(self.image_path) as img:
                img = self.handle_transparency(img)
                img = self.convert_to_grayscale(img)
                # Choose resizing based on model type
                # Google uses media_resolution, Anthropic uses resize_profile, OpenAI uses llm_detail
                if self.model_type == "google":
                    detail = (self.img_cfg.get('media_resolution', 'high') or 'high')
                elif self.model_type == "anthropic":
                    detail = (self.img_cfg.get('resize_profile', 'auto') or 'auto')
                else:
                    detail = (self.img_cfg.get('llm_detail', 'high') or 'high')
                img = ImageProcessor.resize_for_detail(img, detail, self.img_cfg, self.model_type)

                # Force output to JPEG with configurable quality (regardless of extension)
                # Create a new path with .jpg extension
                jpg_output_path = output_path.with_suffix('.jpg')
                jpeg_quality = int(self.img_cfg.get('jpeg_quality', 95))
                img.save(
                    jpg_output_path,
                    format='JPEG',
                    quality=jpeg_quality
                )
                logger.debug(
                    f"Saved processed image {jpg_output_path.name} size={img.size} quality={jpeg_quality} detail={detail}"
                )
            return f"Processed and saved: {jpg_output_path.name}"
        except Exception as e:
            logger.error(f"Error processing image {self.image_path.name}: {e}")
            return f"Failed to process {self.image_path.name}: {e}"

    # --- Static Methods for Folder-Level Processing ---

    @staticmethod
    def prepare_image_folder(folder: Path, image_output_dir: Path) -> Tuple[
        Path, Path, Path, Path]:
        """
        Prepares the output directories for processing an image folder.

        Returns a tuple of:
          - parent_folder: The directory for outputs related to this folder.
          - preprocessed_folder: Where preprocessed images will be stored.
          - temp_jsonl_path: File for recording transcription logs.
          - output_txt_path: Final transcription text file.
        """
        # Use the new path_utils to create a safe directory name with hash
        # The directory name will be truncated with hash if too long
        safe_dir_name = create_safe_directory_name(folder.name)
        
        # Create parent folder with safe directory name
        parent_folder = image_output_dir / safe_dir_name
        parent_folder.mkdir(parents=True, exist_ok=True)
        
        # Create preprocessed images subfolder
        preprocessed_folder = parent_folder / "preprocessed_images"
        preprocessed_folder.mkdir(exist_ok=True)
        
        # Create safe filenames (truncated with hash if needed, considering full path length)
        # No _transcription suffix to keep filenames shorter
        temp_jsonl_name = create_safe_filename(folder.name, ".jsonl", parent_folder)
        output_txt_name = create_safe_filename(folder.name, ".txt", parent_folder)
        
        temp_jsonl_path = parent_folder / temp_jsonl_name
        if not temp_jsonl_path.exists():
            temp_jsonl_path.touch()
        output_txt_path = parent_folder / output_txt_name
        
        return parent_folder, preprocessed_folder, temp_jsonl_path, output_txt_path

    @staticmethod
    def process_images_multiprocessing(image_paths: List[Path],
                                       output_paths: List[Path],
                                       provider: str = "openai",
                                       model_name: str = "") -> List[Optional[str]]:
        """
        Process images using multiprocessing.

        Parameters:
            image_paths (List[Path]): List of source image paths.
            output_paths (List[Path]): List of destination paths for processed images.
            provider (str): Provider name for config selection.
            model_name (str): Model name for detecting underlying model type.

        Returns:
            List[Optional[str]]: Processing results for each image.
        """
        # Load concurrency settings
        conc = get_config_service().get_concurrency_config()
        img_conc = (conc.get('concurrency', {})
                         .get('image_processing', {}))
        processes = int(img_conc.get('concurrency_limit', 12))

        # Create args list with provider and model_name for each image
        args_list = [(img_path, out_path, provider, model_name) 
                     for img_path, out_path in zip(image_paths, output_paths)]
        results = run_multiprocessing_tasks(
            ImageProcessor._process_image_task,
            args_list,
            processes=processes
        )
        return results

    @staticmethod
    def _process_image_task(img_path: Path, out_path: Path, provider: str, model_name: str) -> str:
        """
        Helper task to process a single image and save it to out_path.

        Parameters:
            img_path: Source image path.
            out_path: Destination path for processed image.
            provider: Provider name for config selection.
            model_name: Model name for detecting underlying model type.

        Returns:
            str: A status message.
        """
        processor = ImageProcessor(img_path, provider=provider, model_name=model_name)
        return processor.process_image(out_path)

    @staticmethod
    def process_and_save_images(source_folder: Path, preprocessed_folder: Path,
                                provider: str = "openai", model_name: str = "") -> List[Path]:
        """
        Reads images from source folder, processes them, and saves directly to preprocessed folder.
        Eliminates the need for intermediate raw image storage.

        Parameters:
            source_folder (Path): Path to the source folder containing original images.
            preprocessed_folder (Path): Path to save processed images.
            provider (str): Provider name for config selection.
            model_name (str): Model name for detecting underlying model type.

        Returns:
            List[Path]: List of processed image paths.
        """
        # Find all image files in source folder
        image_files: List[Path] = []
        for ext in SUPPORTED_IMAGE_EXTENSIONS:
            image_files.extend(list(source_folder.glob(f'*{ext}')))

        if not image_files:
            return []

        # Create list of output paths
        output_paths = [
            preprocessed_folder / f"{img.stem}_pre_processed.jpg" for img in image_files
        ]

        # Process all images at once using multiprocessing with provider info
        ImageProcessor.process_images_multiprocessing(image_files, output_paths, provider, model_name)

        # Return all processed image paths that exist
        return [p for p in output_paths if p.exists()]

    # ================== Tesseract-specific preprocessing ==================

    @staticmethod
    def _pil_to_np(image: Image.Image) -> np.ndarray:
        if image.mode == 'RGBA':
            # Flatten alpha onto white
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])
            image = background
        elif image.mode == 'P' and 'transparency' in image.info:
            background = Image.new('RGB', image.size, (255, 255, 255))
            background.paste(image, mask=image.split()[-1])
            image = background
        if image.mode == 'RGB':
            arr = np.array(image)
            return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)
        elif image.mode in ('L', 'I;16'):
            return np.array(image)
        else:
            return cv2.cvtColor(np.array(image.convert('RGB')), cv2.COLOR_RGB2BGR)

    @staticmethod
    def _ensure_grayscale(img: np.ndarray) -> np.ndarray:
        if img.ndim == 3:
            return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        return img

    @staticmethod
    def _auto_invert_if_needed(gray: np.ndarray) -> np.ndarray:
        # Estimate background by sampling 10-px border
        h, w = gray.shape[:2]
        border = 10
        border = min(border, h // 4, w // 4) if h > 0 and w > 0 else 0
        if border > 0:
            top = gray[:border, :]
            bottom = gray[-border:, :]
            left = gray[:, :border]
            right = gray[:, -border:]
            bg_mean = np.mean(np.concatenate([top.flatten(), bottom.flatten(), left.flatten(), right.flatten()]))
        else:
            bg_mean = np.mean(gray)
        # If background is dark (mean < 127), invert to make it light
        if bg_mean < 127:
            return cv2.bitwise_not(gray)
        return gray

    @staticmethod
    def _deskew(gray: np.ndarray) -> Tuple[np.ndarray, float]:
        try:
            angle = determine_skew(gray)
            if angle is None:
                return gray, 0.0
            angle = float(angle)
            if abs(angle) < 0.1:
                return gray, angle
            h, w = gray.shape[:2]
            center = (w // 2, h // 2)
            m = cv2.getRotationMatrix2D(center, angle, 1.0)
            rotated = cv2.warpAffine(gray, m, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=255)
            return rotated, angle
        except Exception:
            return gray, 0.0

    @staticmethod
    def _denoise(gray: np.ndarray, method: str) -> np.ndarray:
        m = (method or 'none').lower()
        if m == 'median':
            return cv2.medianBlur(gray, 3)
        if m == 'bilateral':
            return cv2.bilateralFilter(gray, 9, 75, 75)
        return gray

    @staticmethod
    def _binarize(gray: np.ndarray, method: str, window: int, k: float) -> np.ndarray:
        m = (method or 'sauvola').lower()
        if m == 'sauvola':
            w = window if isinstance(window, int) and window % 2 == 1 else 25
            thresh = threshold_sauvola(gray, window_size=w, k=float(k) if k is not None else 0.2)
            binary = (gray > thresh).astype(np.uint8) * 255
            return binary
        if m in ('adaptive', 'adaptive_otsu', 'adaptive-gaussian'):
            # Use Gaussian adaptive threshold
            return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                         cv2.THRESH_BINARY, 25, 10)
        # Otsu as default fallback
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return binary

    @staticmethod
    def _morph(binary: np.ndarray, op: str, ksize: int) -> np.ndarray:
        opn = (op or 'none').lower()
        if opn == 'none':
            return binary
        k = max(1, int(ksize))
        if k % 2 == 0:
            k += 1
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (k, k))
        if opn == 'open':
            return cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        if opn == 'close':
            return cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        if opn == 'erode':
            return cv2.erode(binary, kernel, iterations=1)
        if opn == 'dilate':
            return cv2.dilate(binary, kernel, iterations=1)
        return binary

    @staticmethod
    def preprocess_for_tesseract(image: Image.Image, cfg: dict) -> Tuple[Image.Image, dict]:
        """
        Preprocess a PIL image for Tesseract OCR according to cfg.
        Returns (PIL.Image in 'L' mode with 0/255, diagnostics dict)
        """
        diag = {}
        # 1) Flatten alpha and to np
        np_img = ImageProcessor._pil_to_np(image)
        # 2) Grayscale
        gray = ImageProcessor._ensure_grayscale(np_img)
        # 3) Polarity
        invert_mode = str(cfg.get('invert_to_dark_on_light', 'auto')).lower()
        if invert_mode == 'always':
            gray = cv2.bitwise_not(gray)
            diag['inverted'] = True
        elif invert_mode == 'auto':
            before_mean = float(np.mean(gray)) if gray.size else 0.0
            gray = ImageProcessor._auto_invert_if_needed(gray)
            after_mean = float(np.mean(gray)) if gray.size else 0.0
            diag['inverted'] = after_mean > before_mean
        else:
            diag['inverted'] = False
        # 4) Deskew
        if bool(cfg.get('deskew', True)):
            gray, angle = ImageProcessor._deskew(gray)
            diag['skew_angle'] = float(angle)
        else:
            diag['skew_angle'] = 0.0
        # 5) Denoise
        gray = ImageProcessor._denoise(gray, cfg.get('denoise', 'median'))
        # 6) Binarization
        binary = ImageProcessor._binarize(
            gray,
            cfg.get('binarization', 'sauvola'),
            int(cfg.get('sauvola_window', 25)),
            float(cfg.get('sauvola_k', 0.2)),
        )
        diag['binarization'] = cfg.get('binarization', 'sauvola')
        # 7) Morphology
        binary = ImageProcessor._morph(binary, cfg.get('morphology', 'none'), int(cfg.get('morph_kernel', 3)))
        diag['morphology'] = cfg.get('morphology', 'none')
        # 8) Border
        b = int(cfg.get('border_px', 10))
        if b and b > 0:
            binary = cv2.copyMakeBorder(binary, b, b, b, b, cv2.BORDER_CONSTANT, value=255)
        # Convert back to PIL 'L'
        pil_bin = Image.fromarray(binary)
        if pil_bin.mode != 'L':
            pil_bin = pil_bin.convert('L')
        return pil_bin, diag

    @staticmethod
    def process_and_save_images_for_tesseract(source_folder: Path, preprocessed_folder: Path) -> List[Path]:
        """
        Process images for Tesseract (lossless, full resolution) and save as PNG/TIFF.
        """
        config_service = get_config_service()
        tip_cfg = config_service.get_image_processing_config().get('tesseract_image_processing', {})
        preproc_cfg = tip_cfg.get('preprocessing', {})
        output_format = str(preproc_cfg.get('output_format', 'png')).lower()
        target_dpi = int(tip_cfg.get('target_dpi', 300))
        embed_dpi = bool(preproc_cfg.get('embed_dpi_metadata', True))
        # Concurrency settings
        conc = config_service.get_concurrency_config()
        img_conc = (conc.get('concurrency', {})
                         .get('image_processing', {}))
        processes = int(img_conc.get('concurrency_limit', 4))

        # Find all images
        image_files: List[Path] = []
        for ext in SUPPORTED_IMAGE_EXTENSIONS:
            image_files.extend(list(source_folder.glob(f'*{ext}')))
        if not image_files:
            return []

        # Deterministic ordering by filename
        image_files.sort(key=lambda p: p.name)

        preprocessed_folder.mkdir(parents=True, exist_ok=True)
        suffix = '.png' if output_format == 'png' else '.tif'
        out_paths: List[Path] = [
            preprocessed_folder / f"{img.stem}_tess_preprocessed{suffix}"
            for img in image_files
        ]

        # Build args for multiprocessing
        args_list = [
            (img_path, out_path, preproc_cfg, output_format, target_dpi, embed_dpi)
            for img_path, out_path in zip(image_files, out_paths)
        ]

        # Run in process pool honoring concurrency config
        run_multiprocessing_tasks(
            ImageProcessor._tesseract_preprocess_image_task,
            args_list,
            processes=processes,
        )

        # Return files that were successfully written
        return [p for p in out_paths if p.exists()]

    @staticmethod
    def _tesseract_preprocess_image_task(img_path: Path, out_path: Path, cfg: dict,
                                         output_format: str, target_dpi: int,
                                         embed_dpi: bool) -> Optional[str]:
        """Worker: open, preprocess with Tesseract pipeline, and save losslessly."""
        try:
            with Image.open(img_path) as im:
                processed_img, diag = ImageProcessor.preprocess_for_tesseract(im, cfg)
                save_kwargs = {}
                if embed_dpi:
                    save_kwargs['dpi'] = (target_dpi, target_dpi)
                processed_img.save(out_path, **save_kwargs)
                logger.debug(f"Tesseract-preprocessed {img_path.name} -> {out_path.name} diag={diag}")
                return str(out_path)
        except Exception as e:
            logger.error(f"Error Tesseract-preprocessing image {img_path.name}: {e}")
            return None
